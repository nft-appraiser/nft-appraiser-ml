{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b83eaf3b-a15b-42fd-99da-dc9e6950f7bd",
   "metadata": {},
   "source": [
    "# 価格予測モデルのBaseline  \n",
    "- CNNを用いたモデルを作成する．  \n",
    "- 価格予測とクラス分類でタスクが大きく異なるので，imagenetで学習したモデルを用いないものを最初に作成する．  \n",
    "- サイトに載せられる画像を教師データとしており，画像が大きく回転したりなどは不要と考えられるためそのような前処理は行わない．  \n",
    "- 損失関数にはmaeもしくはrmseを用いる．  \n",
    "\n",
    "## モデルの構築  \n",
    "- EfficientNetB0（未学習）を用いて特徴量を抽出．  \n",
    "- num_sales, コレクション名のone-hotベクトルを抽出した特徴量に結合．  \n",
    "- 全結合層を重ねて出力．  \n",
    "- ImageNetを用いて事前学習したものとしていないもので比較する．  \n",
    "- 目的変数をそのまま予測するとスケールが大きすぎるので，先に対数変換して評価関数にRMSE, MAEなどを用いるほうが良いかも．  \n",
    "- **このノートブックでやっているのは事前学習有り．**  \n",
    "\n",
    "## 評価関数  \n",
    "- RMSLEを用いる．  \n",
    "$$RMSLE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (\\log{(y_i+1)} - \\log{(\\hat{y_i} +1)})^2}$$\n",
    "\n",
    "- 追加でMAPEを用いてみる．  \n",
    "$$MAPE = \\frac{100}{n} \\sum_{i=1}^n |\\frac{\\hat{y}_i - y_i}{y_i}|$$\n",
    "\n",
    "タスクAに関してはデータ不足の可能性が考えられるため，特徴量抽出とともにデータを追加で収集する．  \n",
    "\n",
    "## 変数（タスクA）  \n",
    "- 目的変数: last_sale.total_price  \n",
    "- 説明変数: 画像データ，コレクション名（collection.name），num_sales，\n",
    "\n",
    "## 変数（タスクB）  \n",
    "- 目的変数: last_sale.total_price  \n",
    "- 説明変数: 画像データ  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd6e453-d7a2-447a-94c2-e3e41e7c841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "import math\n",
    "import tempfile\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.losses as losses\n",
    "import tensorflow.keras.optimizers as optim\n",
    "import tensorflow.keras.activations as activations\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "import tensorflow.keras.callbacks as callbacks\n",
    "from tensorflow.keras.applications import EfficientNetB0 as efn\n",
    "import cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa2369f-6c62-4ec4-a3cd-6790e8794ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (3,27,28,71,88,119) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (121,122,123,124) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (21747, 169)\n",
      "data shape: (5200, 177)\n"
     ]
    }
   ],
   "source": [
    "A_IMGPATH = \"../data/taskA/img\"\n",
    "A_DFPATH = \"../data/taskA/table\"\n",
    "B_IMGPATH = \"../data/taskB/img\"\n",
    "B_DFPATH = \"../data/taskB/table\"\n",
    "asset_df_A = pd.read_csv(os.path.join(A_DFPATH, \"asset_data.csv\"))\n",
    "asset_df_B = pd.read_csv(os.path.join(B_DFPATH, \"asset_data.csv\"))\n",
    "\n",
    "asset_df_A = asset_df_A.rename(columns={\"last_sale.total_price\": \"target\"})\n",
    "asset_df_B = asset_df_B.rename(columns={\"last_sale.total_price\": \"target\"})\n",
    "\n",
    "asset_df_A = pd.concat((asset_df_A, pd.get_dummies(asset_df_A[\"collection.name\"])), axis=1)\n",
    "asset_df_B[asset_df_A[\"collection.name\"].unique()] = 0\n",
    "\n",
    "asset_df_A[\"full_path\"] =\\\n",
    "    asset_df_A[\"image_id\"].apply(lambda x: A_IMGPATH + \"/\" + x)\n",
    "asset_df_B[\"full_path\"] =\\\n",
    "    asset_df_B[\"image_id\"].apply(lambda x: B_IMGPATH + \"/\" + x)\n",
    "\n",
    "asset_df_A['target'] = asset_df_A['target'].astype(float) * 1e-18\n",
    "asset_df_B['target'] = asset_df_B['target'].astype(float) * 1e-18\n",
    "asset_df_A = asset_df_A.query('target > 0')\n",
    "asset_df_B = asset_df_B.query('target > 0')\n",
    "asset_df_A['target'] = asset_df_A['target'].apply(lambda x: np.log1p(x))\n",
    "asset_df_B['target'] = asset_df_B['target'].apply(lambda x: np.log1p(x))\n",
    "\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "print(f\"data shape: {asset_df_A.shape}\")\n",
    "print(f\"data shape: {asset_df_B.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9128f-90a3-4a76-a850-c81273b9967d",
   "metadata": {},
   "source": [
    "## Helper Functions  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2126e4f-9af4-4b15-9bde-dc1a5f8a1124",
   "metadata": {},
   "source": [
    "### DataLoader  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb23666-1673-4db8-85da-9f8aa0fcbd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullPathDataLoader(Sequence):\n",
    "    \"\"\"\n",
    "    Data loader that load images, meta data and targets.\n",
    "    This class is inherited Sequence class of Keras.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path_list: np.ndarray, target: Optional[np.ndarray],\n",
    "                 meta_data: Optional[np.ndarray] = None, batch_size: int = 16,\n",
    "                 task: str = \"B\", width: int = 256, height: int = 256,\n",
    "                 resize: bool = True, shuffle: bool = True, is_train: bool = True):\n",
    "        \"\"\"\n",
    "        Constructor. This method determines class variables.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path_list : np.ndarray[str]\n",
    "            The array of absolute paths of images.\n",
    "        meta_data : np.ndarray[int]\n",
    "            One-hot vector of collections.\n",
    "        target : np.ndarray\n",
    "            Array of target variavles.\n",
    "        batch_size : int\n",
    "            Batch size used when model training.\n",
    "        task : str\n",
    "            Please determine this data loader will be used for task A or B(default=A).\n",
    "        width : int\n",
    "            Width of resized image.\n",
    "        height : int\n",
    "            Height of resize image.\n",
    "        resize : bool\n",
    "            Flag determine whether to resize.\n",
    "        shuffle : bool\n",
    "            Flag determine whether to shuffle on epoch end.\n",
    "        is_train : bool\n",
    "            Determine whether this data loader will be used training model.\n",
    "            if you won't this data loader, you have set 'is_train'=False.\n",
    "        \"\"\"\n",
    "        self.path_list = path_list\n",
    "        self.batch_size = batch_size\n",
    "        self.task = task\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.resize = resize\n",
    "        self.shuffle = shuffle\n",
    "        self.is_train = is_train\n",
    "        self.length = math.ceil(len(self.path_list) / self.batch_size)\n",
    "\n",
    "        if self.is_train:\n",
    "            self.target = target\n",
    "        if self.task == \"A\":\n",
    "            self.meta_data = meta_data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        self.length : data length\n",
    "        \"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def get_img(self, path_list: np.ndarray):\n",
    "        \"\"\"\n",
    "        Load image data and resize image if 'resize'=True.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path_liist : np.ndarray\n",
    "            The array of relative image paths from directory 'dir_name'.\n",
    "            Size of this array is 'batch_size'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        img_list : np.ndarray\n",
    "            The array of image data.\n",
    "            Size of an image is (width, height, 3) if 'resize'=True.\n",
    "        '\"\"\"\n",
    "        img_list = []\n",
    "        for path in path_list:\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.resize(img, (self.width, self.height))\n",
    "            img = img / 255.\n",
    "            img_list.append(img)\n",
    "\n",
    "        img_list = np.array(img_list)\n",
    "        return img_list\n",
    "\n",
    "    def _shuffle(self):\n",
    "        \"\"\"\n",
    "        Shuffle path_list, meta model.\n",
    "        If 'is_train' is True, target is shuffled in association path_list.\n",
    "        \"\"\"\n",
    "        idx = np.random.permutation(len(self.path_list))\n",
    "        self.path_list = self.path_list[idx]\n",
    "        if self.task == \"A\":\n",
    "            self.meta_data = self.meta_data[idx]\n",
    "        if self.is_train:\n",
    "            self.target = self.target[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_list = self.path_list[self.batch_size*idx:self.batch_size*(idx+1)]\n",
    "        img_list = self.get_img(path_list)\n",
    "        if self.is_train:\n",
    "            target_list = self.target[self.batch_size*idx:self.batch_size*(idx+1)]\n",
    "            if self.task == \"A\":\n",
    "                meta = self.meta_data[self.batch_size*idx:self.batch_size*(idx+1)]\n",
    "                return (img_list, meta), target_list\n",
    "            else:\n",
    "                return img_list, target_list\n",
    "        else:\n",
    "            if self.task == \"A\":\n",
    "                meta = self.meta_data[self.batch_size*idx:self.batch_size*(idx+1)]\n",
    "                return ((img_list, meta),)\n",
    "            else:\n",
    "                return img_list\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.is_train:\n",
    "            self._shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccabc2f6-749f-404a-a617-9f0507113d78",
   "metadata": {},
   "source": [
    "### seed settings  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44fab9da-4ce3-4548-ba52-29baf9c5e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(random_state=6174):\n",
    "    tf.random.set_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "    random.seed(random_state)\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab18eb9-6e4f-40f8-9ffc-c2b394b47870",
   "metadata": {},
   "source": [
    "### Create model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae81ae9f-aead-43ec-9c7b-906a40574366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape: Tuple[int], output_shape: int,\n",
    "                 activation, loss, meta_shape: Optional[int] = None,\n",
    "                 task: str = \"B\", learning_rate: float = 0.001,\n",
    "                 pretrain: bool = False) -> models.Model:\n",
    "    \"\"\"\n",
    "    The function for creating model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : int\n",
    "        Shape of input image data.\n",
    "    output_shape : int\n",
    "        Shape of model output.\n",
    "    activation : function\n",
    "        The activation function used hidden layers.\n",
    "    loss : function\n",
    "        The loss function of model.\n",
    "    meta_shape : int\n",
    "        Shape of input meta data of image.\n",
    "    task : str\n",
    "        Please determine this model will be used for task A or B(default=A).\n",
    "    learning_rate : float\n",
    "        The learning rate of model.\n",
    "    pretrain : bool\n",
    "        Flag that deterimine whether use pretrain model(default=False).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : keras.models.Model\n",
    "        Model instance.\n",
    "    \"\"\"\n",
    "    if pretrain:\n",
    "        weights = 'imagenet'\n",
    "    else:\n",
    "        weights = None\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    efn_model = efn(include_top=False, input_shape=input_shape,\n",
    "                    weights=weights)(inputs)\n",
    "    ga = layers.GlobalAveragePooling2D()(efn_model)\n",
    "\n",
    "    if task == \"A\":\n",
    "        meta_inputs = layers.Input(shape=meta_shape)\n",
    "        concate = layers.Concatenate()([ga, meta_inputs])\n",
    "        dense1 = layers.Dense(units=128)(concate)\n",
    "        av1 = layers.Activation(activation)(dense1)\n",
    "        dr1 = layers.Dropout(0.3)(av1)\n",
    "        dense2 = layers.Dense(units=64)(dr1)\n",
    "        av2 = layers.Activation(activation)(dense2)\n",
    "        dr2 = layers.Dropout(0.3)(av2)\n",
    "        outputs = layers.Dense(output_shape)(dr2)\n",
    "\n",
    "        model = models.Model(inputs=[inputs, meta_inputs], outputs=[outputs])\n",
    "\n",
    "    elif task == \"B\":\n",
    "        dense1 = layers.Dense(units=128)(ga)\n",
    "        av1 = layers.Activation(activation)(dense1)\n",
    "        dr1 = layers.Dropout(0.3)(av1)\n",
    "        dense2 = layers.Dense(units=64)(dr1)\n",
    "        av2 = layers.Activation(activation)(dense2)\n",
    "        dr2 = layers.Dropout(0.3)(av2)\n",
    "        outputs = layers.Dense(output_shape)(dr2)\n",
    "\n",
    "        model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Please set task is A or B.\")\n",
    "\n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=optim.SGD(learning_rate=learning_rate, momentum=0.9),\n",
    "                  metrics=['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d8ef5c-83cf-4e06-a76a-ebac170eab62",
   "metadata": {},
   "source": [
    "### Training model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d69c795d-f36c-45e1-88b4-f3c591e4f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(path_list: np.ndarray, target: np.ndarray, loss,\n",
    "          meta_data: Optional[np.ndarray] = None, task: str = \"B\"):\n",
    "    \"\"\"\n",
    "    The function for training model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_list : np.ndarray\n",
    "        The path list of all image data.\n",
    "    target : np.ndarray\n",
    "        The array of targets data.\n",
    "    loss : function\n",
    "        The loss function of keras.\n",
    "    meta_data : np.ndarray\n",
    "        The array of meta data of image.\n",
    "    task : str\n",
    "        Please determine you train model for task A or B(default=A).\n",
    "    \"\"\"\n",
    "    if task == \"A\":\n",
    "        train_path, val_path, train_meta, val_meta, train_y, val_y =\\\n",
    "            train_test_split(path_list, meta_data, target, test_size=0.1, random_state=6174)\n",
    "        train_gen = FullPathDataLoader(path_list=train_path, target=train_y,\n",
    "                                       meta_data=train_meta, batch_size=16,\n",
    "                                       task=task)\n",
    "        val_gen = FullPathDataLoader(path_list=val_path, target=train_y,\n",
    "                                     meta_data=val_meta, batch_size=1,\n",
    "                                     task=task)\n",
    "    elif task == \"B\":\n",
    "        train_path, val_path, train_y, val_y =\\\n",
    "            train_test_split(path_list, target, test_size=0.1, random_state=6174)\n",
    "        train_gen = FullPathDataLoader(path_list=train_path, target=train_y,\n",
    "                                       batch_size=16, task=task)\n",
    "        val_gen = FullPathDataLoader(path_list=val_path, target=val_y,\n",
    "                                     batch_size=1, task=task)\n",
    "    else:\n",
    "        raise Exception(\"Please set task is A or B\")\n",
    "\n",
    "    set_seed()\n",
    "    model = NFTModel(\n",
    "        create_model(input_shape=(256, 256, 3), output_shape=1,\n",
    "                     activation=activations.relu, loss=loss,\n",
    "                     meta_shape=len(meta_features), task=task,\n",
    "                     learning_rate=0.00001, pretrain=True)\n",
    "    )\n",
    "\n",
    "    ES = callbacks.EarlyStopping(monitor='val_loss', patience=10,\n",
    "                                 restore_best_weights=True)\n",
    "\n",
    "    print(\"starting training\")\n",
    "    print('*' + '-' * 30 + '*')\n",
    "\n",
    "    model.fit(train_gen, val_gen, epochs=100, batch_size=16,\n",
    "              callbacks=[ES])\n",
    "\n",
    "    print(\"finished training\")\n",
    "    print('*' + '-' * 30 + '*' + '\\n')\n",
    "\n",
    "    if task == \"A\":\n",
    "        val_gen = FullPathDataLoader(path_list=val_path, target=train_y,\n",
    "                                     meta_data=val_meta, batch_size=1, task=task,\n",
    "                                     shuffle=False, is_train=False)\n",
    "    else:\n",
    "        val_gen = FullPathDataLoader(path_list=val_path, target=train_y,\n",
    "                                     batch_size=1, task=task,\n",
    "                                     shuffle=False, is_train=False)\n",
    "    print(\"starting evaluate\")\n",
    "    print('*' + '-' * 30 + '*')\n",
    "\n",
    "    model.evaluate(val_gen, val_y)\n",
    "\n",
    "    print(\"finished evaluate\")\n",
    "    print('*' + '-' * 30 + '*' + '\\n')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8611a18a-51e3-4841-ac3b-3b3400d8ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFTModel(KerasRegressor):\n",
    "    \"\"\"\n",
    "    Model class.\n",
    "    This class is inherited KerasRegressor class of keras.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_func):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        Prameters\n",
    "        ---------\n",
    "        model_func : function\n",
    "            The function for creating model.\n",
    "        \"\"\"\n",
    "        super().__init__(build_fn=model_func)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        result = {'sk_params': self.sk_params}\n",
    "        with tempfile.TemporaryDirectory() as dir:\n",
    "            if hasattr(self, 'model'):\n",
    "                self.model.save(dir + '/output.h5', include_optimizer=False)\n",
    "                with open(dir + '/output.h5', 'rb') as f:\n",
    "                    result['model'] = f.read()\n",
    "        return result\n",
    "\n",
    "    def __setstate__(self, serialized):\n",
    "        self.sk_params = serialized['sk_params']\n",
    "        with tempfile.TemporaryDirectory() as dir:\n",
    "            model_data = serialized.get('model')\n",
    "            if model_data:\n",
    "                with open(dir + '/input.h5', 'wb') as f:\n",
    "                    f.write(model_data)\n",
    "                self.model = tf.keras.models.load_model(dir + '/input.h5')\n",
    "\n",
    "    def fit(self, train_gen, val_gen, epochs, batch_size, callbacks):\n",
    "        \"\"\"\n",
    "        Training model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_gen : iterator\n",
    "            The generator of train data.\n",
    "        val_gen : iterator\n",
    "            The generator of validation data.\n",
    "        epochs : int\n",
    "            Number of epochs for training model.\n",
    "        batch_size : int\n",
    "            Size of batch for training model.\n",
    "        callbacks : list\n",
    "            The list of callbacks.\n",
    "            For example [EarlyStopping instance, ModelCheckpoint instance]\n",
    "        \"\"\"\n",
    "        self.model = self.build_fn\n",
    "        self.model.fit(train_gen, epochs=epochs, batch_size=batch_size,\n",
    "                       validation_data=val_gen, callbacks=callbacks)\n",
    "\n",
    "    def evaluate(self, test_X, test_y):\n",
    "        \"\"\"\n",
    "        Evaluate model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        test_X : iterator\n",
    "            The generator of test data.\n",
    "        test_y : np.ndarray\n",
    "            The array of targets of test data.\n",
    "        \"\"\"\n",
    "        pred = self.model.predict(test_X)\n",
    "        pred = np.where(pred < 0, 0, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(test_y, pred))\n",
    "        mae = np.sqrt(mean_absolute_error(test_y, pred))\n",
    "\n",
    "        print(f\"RMSE Score: {rmse}\")\n",
    "        print(f\"MAE Score: {mae}\")\n",
    "\n",
    "    def predict(self, img_path: str, collection_name: str, num_sales: int,\n",
    "                task: str = \"B\"):\n",
    "        \"\"\"\n",
    "        Predict data using trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img_path : str\n",
    "            The path of image data.\n",
    "        collection_name : str\n",
    "            Name of collection of the NFT.\n",
    "        num_sales : int\n",
    "            Number of times the NFT sold.\n",
    "        \"\"\"\n",
    "        if task == \"A\":\n",
    "            collections = ['CryptoPunks',\n",
    "                           'Bored Ape Yacht Club',\n",
    "                           'Edifice by Ben Kovach',\n",
    "                           'Mutant Ape Yacht Club',\n",
    "                           'The Sandbox',\n",
    "                           'Divine Anarchy',\n",
    "                           'Cosmic Labs',\n",
    "                           'Parallel Alpha',\n",
    "                           'Art Wars | AW',\n",
    "                           'Neo Tokyo Identities',\n",
    "                           'Neo Tokyo Part 2 Vault Cards',\n",
    "                           'Cool Cats NFT',\n",
    "                           'CrypToadz by GREMPLIN',\n",
    "                           'BearXLabs',\n",
    "                           'Desperate ApeWives',\n",
    "                           'Decentraland',\n",
    "                           'Neo Tokyo Part 3 Item Caches',\n",
    "                           'Doodles',\n",
    "                           'The Doge Pound',\n",
    "                           'Playboy Rabbitars Official',\n",
    "                           'THE SHIBOSHIS',\n",
    "                           'THE REAL GOAT SOCIETY',\n",
    "                           'Sipherian Flash',\n",
    "                           'Party Ape | Billionaire Club',\n",
    "                           'Treeverse',\n",
    "                           'Angry Apes United',\n",
    "                           'CyberKongz',\n",
    "                           'Emblem Vault [Ethereum]',\n",
    "                           'Fat Ape Club',\n",
    "                           'VeeFriends',\n",
    "                           'JUNGLE FREAKS BY TROSLEY',\n",
    "                           'Meebits',\n",
    "                           'Furballs.com Official',\n",
    "                           'Kaiju Kingz',\n",
    "                           'Bears Deluxe',\n",
    "                           'PUNKS Comic',\n",
    "                           'Hor1zon Troopers',\n",
    "                           'Lazy Lions',\n",
    "                           'LOSTPOETS',\n",
    "                           'Chain Runners',\n",
    "                           'Chromie Squiggle by Snowfro',\n",
    "                           'MekaVerse',\n",
    "                           'Vox Collectibles',\n",
    "                           'MutantCats',\n",
    "                           'World of Women',\n",
    "                           'SuperFarm Genesis Series',\n",
    "                           'Eponym by ART AI',]\n",
    "            collection_dict = {\n",
    "                 collections[i]: i for i in range(len(collections))\n",
    "            }\n",
    "            meta_data = np.zeros(shape=(len(collection_dict)+1))\n",
    "            if collection_name in collection_dict.keys():\n",
    "                meta_data[collection_dict[collection_name]] = 1\n",
    "            meta_data[-1] = num_sales\n",
    "            meta_data = meta_data.reshape(1, -1)\n",
    "\n",
    "            img = cv2.resize(cv2.imread(img_path)/256., (256, 256))\n",
    "            img = img.reshape(1, 256, 256, 3)\n",
    "\n",
    "            pred = self.model.predict([img, meta_data])\n",
    "        elif task == \"B\":\n",
    "            img = cv2.resize(cv2.imread(img_path)/256., (256, 256))\n",
    "            img = img.reshape(1, 256, 256, 3)\n",
    "\n",
    "            pred = self.model.predict(img)\n",
    "        else:\n",
    "            raise Exception(\"Please set task is A or B\")\n",
    "\n",
    "        return pred[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "184a4688-00bf-47b8-8e57-635cb9f0f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(instance, file_name: str):\n",
    "    \"\"\"\n",
    "    Save model as pickle file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    instance : Class instance\n",
    "        The class instance you want to save as pickle file.\n",
    "    file_name : str\n",
    "        The absolute path of file saved the instance.\n",
    "    \"\"\"\n",
    "    with open(file_name, mode='wb') as f:\n",
    "        cloudpickle.dump(instance, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9261d8b-47c3-4e1c-8bfa-f98763d74a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file_name: str):\n",
    "    \"\"\"\n",
    "    Load the model file of pickle.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        The absolute path of the model file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : tf.keras.models.Model\n",
    "        Trained model object.\n",
    "    \"\"\"\n",
    "    with open(file_name, mode='rb') as f:\n",
    "        model = cloudpickle.load(f)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b4cb72-e675-4a37-93d2-c76038d38b58",
   "metadata": {},
   "source": [
    "## Training models  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3837f2-893c-4766-b986-38c62827e53b",
   "metadata": {},
   "source": [
    "### TaskA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad0327f9-d21b-4b89-b863-43c45eec513b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-14 08:31:09.150668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-14 08:31:09.155139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-14 08:31:09.155625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-14 08:31:09.156568: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-14 08:31:09.157185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-14 08:31:09.157740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-14 08:31:09.158165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-14 08:31:09.430405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-14 08:31:09.430815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-14 08:31:09.431178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-14 08:31:09.431526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9809 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "*------------------------------*\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-14 08:31:10.735273: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-11-14 08:31:14.745896: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8204\n",
      "2021-11-14 08:31:16.235274: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1224/1224 [==============================] - 226s 180ms/step - loss: 6.9404 - mae: 1.2429 - mse: 6.9404 - val_loss: 5.3435 - val_mae: 1.0126 - val_mse: 5.3435\n",
      "Epoch 2/100\n",
      "1224/1224 [==============================] - 214s 174ms/step - loss: 3.5293 - mae: 0.9540 - mse: 3.5293 - val_loss: 2.9317 - val_mae: 0.9309 - val_mse: 2.9317\n",
      "Epoch 3/100\n",
      "1224/1224 [==============================] - 215s 176ms/step - loss: 1.8858 - mae: 0.8630 - mse: 1.8858 - val_loss: 2.2883 - val_mae: 0.9268 - val_mse: 2.2883\n",
      "Epoch 4/100\n",
      "1224/1224 [==============================] - 215s 175ms/step - loss: 1.5454 - mae: 0.8143 - mse: 1.5454 - val_loss: 2.1529 - val_mae: 0.9906 - val_mse: 2.1529\n",
      "Epoch 5/100\n",
      "1224/1224 [==============================] - 218s 178ms/step - loss: 1.3273 - mae: 0.7553 - mse: 1.3273 - val_loss: 40.6606 - val_mae: 4.4170 - val_mse: 40.6606\n",
      "Epoch 6/100\n",
      "1224/1224 [==============================] - 215s 176ms/step - loss: 1.2829 - mae: 0.7280 - mse: 1.2829 - val_loss: 4.8667 - val_mae: 1.2190 - val_mse: 4.8667\n",
      "Epoch 7/100\n",
      "1224/1224 [==============================] - 218s 178ms/step - loss: 1.2008 - mae: 0.6964 - mse: 1.2008 - val_loss: 8.2581 - val_mae: 1.6961 - val_mse: 8.2581\n",
      "Epoch 8/100\n",
      "1224/1224 [==============================] - 217s 177ms/step - loss: 1.1648 - mae: 0.6794 - mse: 1.1648 - val_loss: 14.5438 - val_mae: 2.3977 - val_mse: 14.5438\n",
      "Epoch 9/100\n",
      "1224/1224 [==============================] - 217s 178ms/step - loss: 1.1408 - mae: 0.6621 - mse: 1.1408 - val_loss: 2.0490 - val_mae: 0.9079 - val_mse: 2.0490\n",
      "Epoch 10/100\n",
      "1224/1224 [==============================] - 218s 178ms/step - loss: 1.0961 - mae: 0.6456 - mse: 1.0961 - val_loss: 2.0894 - val_mae: 0.9866 - val_mse: 2.0894\n",
      "Epoch 11/100\n",
      "1224/1224 [==============================] - 217s 177ms/step - loss: 1.0765 - mae: 0.6331 - mse: 1.0765 - val_loss: 2.1444 - val_mae: 0.9215 - val_mse: 2.1444\n",
      "Epoch 12/100\n",
      "1224/1224 [==============================] - 217s 177ms/step - loss: 1.0568 - mae: 0.6226 - mse: 1.0568 - val_loss: 2.1241 - val_mae: 0.9346 - val_mse: 2.1241\n",
      "Epoch 13/100\n",
      "1224/1224 [==============================] - 216s 176ms/step - loss: 1.0303 - mae: 0.6108 - mse: 1.0303 - val_loss: 89.0652 - val_mae: 6.8566 - val_mse: 89.0652\n",
      "Epoch 14/100\n",
      "1224/1224 [==============================] - 218s 178ms/step - loss: 0.9790 - mae: 0.5963 - mse: 0.9790 - val_loss: 2.9108 - val_mae: 1.1376 - val_mse: 2.9108\n",
      "Epoch 15/100\n",
      "1224/1224 [==============================] - 218s 178ms/step - loss: 0.9844 - mae: 0.5957 - mse: 0.9844 - val_loss: 2.0798 - val_mae: 0.9084 - val_mse: 2.0798\n",
      "Epoch 16/100\n",
      "1224/1224 [==============================] - 217s 177ms/step - loss: 0.9791 - mae: 0.5884 - mse: 0.9791 - val_loss: 3.6462 - val_mae: 1.1836 - val_mse: 3.6462\n",
      "Epoch 17/100\n",
      "1224/1224 [==============================] - 217s 178ms/step - loss: 0.9764 - mae: 0.5767 - mse: 0.9764 - val_loss: 2.2676 - val_mae: 0.9865 - val_mse: 2.2676\n",
      "Epoch 18/100\n",
      "1224/1224 [==============================] - 217s 177ms/step - loss: 0.9461 - mae: 0.5702 - mse: 0.9461 - val_loss: 2.1036 - val_mae: 0.9313 - val_mse: 2.1036\n",
      "Epoch 19/100\n",
      "1224/1224 [==============================] - 218s 178ms/step - loss: 0.9412 - mae: 0.5689 - mse: 0.9412 - val_loss: 131.4304 - val_mae: 7.1930 - val_mse: 131.4304\n",
      "finished training\n",
      "*------------------------------*\n",
      "\n",
      "starting evaluate\n",
      "*------------------------------*\n",
      "RMSE Score: 1.4561724464322845\n",
      "MAE Score: 0.9523819673445525\n",
      "finished evaluate\n",
      "*------------------------------*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_features =\\\n",
    "    asset_df_A['collection.name'].unique().tolist() + ['num_sales']\n",
    "\n",
    "path_list = asset_df_A['full_path'].values\n",
    "meta_data = asset_df_A[meta_features].values\n",
    "target = asset_df_A['target'].values\n",
    "\n",
    "model_A = train(path_list, target, losses.mean_squared_error, meta_data,\n",
    "                task=\"A\")\n",
    "# save_model(model_A, \"../models/baselineA.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4e9dc9-b1eb-462a-bd8c-a8b559d24e0d",
   "metadata": {},
   "source": [
    "### TaskA（画像のみ）  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcb07da4-d972-487b-90c1-a7d45e057d1f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "*------------------------------*\n",
      "Epoch 1/100\n",
      "1224/1224 [==============================] - 217s 175ms/step - loss: 2.7349 - mae: 1.0369 - mse: 2.7349 - val_loss: 2.2998 - val_mae: 0.9054 - val_mse: 2.2998\n",
      "Epoch 2/100\n",
      "1224/1224 [==============================] - 211s 172ms/step - loss: 1.7788 - mae: 0.8671 - mse: 1.7788 - val_loss: 2.3431 - val_mae: 0.9206 - val_mse: 2.3431\n",
      "Epoch 3/100\n",
      "1224/1224 [==============================] - 213s 174ms/step - loss: 1.4670 - mae: 0.7940 - mse: 1.4670 - val_loss: 14.4225 - val_mae: 1.9665 - val_mse: 14.4225\n",
      "Epoch 4/100\n",
      "1224/1224 [==============================] - 211s 173ms/step - loss: 1.3555 - mae: 0.7492 - mse: 1.3555 - val_loss: 1.9551 - val_mae: 0.9155 - val_mse: 1.9551\n",
      "Epoch 5/100\n",
      "1224/1224 [==============================] - 211s 173ms/step - loss: 1.2367 - mae: 0.7087 - mse: 1.2367 - val_loss: 53.2000 - val_mae: 4.1408 - val_mse: 53.2000\n",
      "Epoch 6/100\n",
      "1224/1224 [==============================] - 212s 174ms/step - loss: 1.2245 - mae: 0.6949 - mse: 1.2245 - val_loss: 9.0278 - val_mae: 1.8043 - val_mse: 9.0278\n",
      "Epoch 7/100\n",
      "1224/1224 [==============================] - 211s 172ms/step - loss: 1.1627 - mae: 0.6682 - mse: 1.1627 - val_loss: 1.3537 - val_mae: 0.7341 - val_mse: 1.3537\n",
      "Epoch 8/100\n",
      "1224/1224 [==============================] - 212s 173ms/step - loss: 1.1215 - mae: 0.6564 - mse: 1.1215 - val_loss: 2.0738 - val_mae: 0.9297 - val_mse: 2.0738\n",
      "Epoch 9/100\n",
      "1224/1224 [==============================] - 212s 173ms/step - loss: 1.1207 - mae: 0.6435 - mse: 1.1207 - val_loss: 1.7477 - val_mae: 0.8601 - val_mse: 1.7477\n",
      "Epoch 10/100\n",
      "1224/1224 [==============================] - 211s 172ms/step - loss: 1.0747 - mae: 0.6293 - mse: 1.0747 - val_loss: 2.0650 - val_mae: 0.8836 - val_mse: 2.0650\n",
      "Epoch 11/100\n",
      "1224/1224 [==============================] - 213s 174ms/step - loss: 1.0472 - mae: 0.6181 - mse: 1.0472 - val_loss: 2.0495 - val_mae: 0.8843 - val_mse: 2.0495\n",
      "Epoch 12/100\n",
      "1224/1224 [==============================] - 210s 172ms/step - loss: 1.0379 - mae: 0.6120 - mse: 1.0379 - val_loss: 1.9776 - val_mae: 0.8993 - val_mse: 1.9776\n",
      "Epoch 13/100\n",
      "1224/1224 [==============================] - 210s 172ms/step - loss: 1.0076 - mae: 0.5968 - mse: 1.0076 - val_loss: 3.0278 - val_mae: 1.0280 - val_mse: 3.0278\n",
      "Epoch 14/100\n",
      "1224/1224 [==============================] - 210s 172ms/step - loss: 0.9805 - mae: 0.5888 - mse: 0.9805 - val_loss: 1.8633 - val_mae: 0.8609 - val_mse: 1.8633\n",
      "Epoch 15/100\n",
      "1224/1224 [==============================] - 212s 173ms/step - loss: 0.9796 - mae: 0.5860 - mse: 0.9796 - val_loss: 6.3747 - val_mae: 1.3898 - val_mse: 6.3747\n",
      "Epoch 16/100\n",
      "1224/1224 [==============================] - 211s 172ms/step - loss: 0.9655 - mae: 0.5759 - mse: 0.9655 - val_loss: 1.8551 - val_mae: 0.8143 - val_mse: 1.8551\n",
      "Epoch 17/100\n",
      "1224/1224 [==============================] - 211s 172ms/step - loss: 0.9659 - mae: 0.5754 - mse: 0.9659 - val_loss: 3.7506 - val_mae: 1.1146 - val_mse: 3.7506\n",
      "finished training\n",
      "*------------------------------*\n",
      "\n",
      "starting evaluate\n",
      "*------------------------------*\n",
      "RMSE Score: 1.16349655089246\n",
      "MAE Score: 0.8567837679315085\n",
      "finished evaluate\n",
      "*------------------------------*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_list = asset_df_A['full_path'].values\n",
    "target = asset_df_A['target'].values\n",
    "\n",
    "model_A = train(path_list, target, losses.mean_squared_error,\n",
    "                task=\"B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87444d2c-ae2a-4cc6-9776-6ab420b16ed4",
   "metadata": {},
   "source": [
    "### TaskB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abcc6969-e9b1-4aed-bde1-22975d18fb10",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "*------------------------------*\n",
      "Epoch 1/100\n",
      "293/293 [==============================] - 57s 181ms/step - loss: 0.5716 - mae: 0.5127 - mse: 0.5716 - val_loss: 0.3407 - val_mae: 0.3353 - val_mse: 0.3407\n",
      "Epoch 2/100\n",
      "293/293 [==============================] - 52s 176ms/step - loss: 0.4381 - mae: 0.4247 - mse: 0.4381 - val_loss: 0.3661 - val_mae: 0.2986 - val_mse: 0.3661\n",
      "Epoch 3/100\n",
      "293/293 [==============================] - 52s 177ms/step - loss: 0.3878 - mae: 0.3914 - mse: 0.3878 - val_loss: 0.3510 - val_mae: 0.3146 - val_mse: 0.3510\n",
      "Epoch 4/100\n",
      "293/293 [==============================] - 51s 174ms/step - loss: 0.3690 - mae: 0.3748 - mse: 0.3690 - val_loss: 0.3481 - val_mae: 0.2835 - val_mse: 0.3481\n",
      "Epoch 5/100\n",
      "293/293 [==============================] - 51s 175ms/step - loss: 0.3544 - mae: 0.3763 - mse: 0.3544 - val_loss: 0.3389 - val_mae: 0.3086 - val_mse: 0.3389\n",
      "Epoch 6/100\n",
      "293/293 [==============================] - 51s 173ms/step - loss: 0.3485 - mae: 0.3657 - mse: 0.3485 - val_loss: 0.3289 - val_mae: 0.3119 - val_mse: 0.3289\n",
      "Epoch 7/100\n",
      "293/293 [==============================] - 51s 173ms/step - loss: 0.3376 - mae: 0.3559 - mse: 0.3376 - val_loss: 0.3242 - val_mae: 0.2918 - val_mse: 0.3242\n",
      "Epoch 8/100\n",
      "293/293 [==============================] - 51s 174ms/step - loss: 0.3219 - mae: 0.3496 - mse: 0.3219 - val_loss: 0.3509 - val_mae: 0.3229 - val_mse: 0.3509\n",
      "Epoch 9/100\n",
      "293/293 [==============================] - 51s 174ms/step - loss: 0.3223 - mae: 0.3495 - mse: 0.3223 - val_loss: 0.3283 - val_mae: 0.3215 - val_mse: 0.3283\n",
      "Epoch 10/100\n",
      "293/293 [==============================] - 51s 175ms/step - loss: 0.3272 - mae: 0.3480 - mse: 0.3272 - val_loss: 0.3287 - val_mae: 0.3198 - val_mse: 0.3287\n",
      "Epoch 11/100\n",
      "293/293 [==============================] - 51s 175ms/step - loss: 0.3151 - mae: 0.3407 - mse: 0.3151 - val_loss: 0.3364 - val_mae: 0.3479 - val_mse: 0.3364\n",
      "Epoch 12/100\n",
      "293/293 [==============================] - 52s 176ms/step - loss: 0.3094 - mae: 0.3405 - mse: 0.3094 - val_loss: 0.3221 - val_mae: 0.3068 - val_mse: 0.3221\n",
      "Epoch 13/100\n",
      "293/293 [==============================] - 51s 175ms/step - loss: 0.3039 - mae: 0.3319 - mse: 0.3039 - val_loss: 0.3249 - val_mae: 0.3248 - val_mse: 0.3249\n",
      "Epoch 14/100\n",
      "293/293 [==============================] - 51s 174ms/step - loss: 0.3003 - mae: 0.3327 - mse: 0.3003 - val_loss: 0.3387 - val_mae: 0.3266 - val_mse: 0.3387\n",
      "Epoch 15/100\n",
      "293/293 [==============================] - 51s 174ms/step - loss: 0.3073 - mae: 0.3326 - mse: 0.3073 - val_loss: 0.3018 - val_mae: 0.2656 - val_mse: 0.3018\n",
      "Epoch 16/100\n",
      "293/293 [==============================] - 51s 175ms/step - loss: 0.3016 - mae: 0.3265 - mse: 0.3016 - val_loss: 0.3408 - val_mae: 0.3381 - val_mse: 0.3408\n",
      "Epoch 17/100\n",
      "293/293 [==============================] - 51s 175ms/step - loss: 0.2898 - mae: 0.3267 - mse: 0.2898 - val_loss: 0.3852 - val_mae: 0.4113 - val_mse: 0.3852\n",
      "Epoch 18/100\n",
      "293/293 [==============================] - 51s 175ms/step - loss: 0.2999 - mae: 0.3274 - mse: 0.2999 - val_loss: 0.3065 - val_mae: 0.2868 - val_mse: 0.3065\n",
      "Epoch 19/100\n",
      "293/293 [==============================] - 51s 173ms/step - loss: 0.2872 - mae: 0.3235 - mse: 0.2872 - val_loss: 0.3361 - val_mae: 0.3306 - val_mse: 0.3361\n",
      "Epoch 20/100\n",
      "293/293 [==============================] - 51s 174ms/step - loss: 0.2893 - mae: 0.3199 - mse: 0.2893 - val_loss: 0.3377 - val_mae: 0.3297 - val_mse: 0.3377\n",
      "Epoch 21/100\n",
      "293/293 [==============================] - 51s 174ms/step - loss: 0.2799 - mae: 0.3151 - mse: 0.2799 - val_loss: 0.3188 - val_mae: 0.3051 - val_mse: 0.3188\n",
      "Epoch 22/100\n",
      "293/293 [==============================] - 51s 173ms/step - loss: 0.2814 - mae: 0.3128 - mse: 0.2814 - val_loss: 0.3297 - val_mae: 0.3360 - val_mse: 0.3297\n",
      "Epoch 23/100\n",
      "293/293 [==============================] - 51s 173ms/step - loss: 0.2776 - mae: 0.3141 - mse: 0.2776 - val_loss: 0.3198 - val_mae: 0.3276 - val_mse: 0.3198\n",
      "Epoch 24/100\n",
      "293/293 [==============================] - 51s 173ms/step - loss: 0.2766 - mae: 0.3102 - mse: 0.2766 - val_loss: 0.3612 - val_mae: 0.3689 - val_mse: 0.3612\n",
      "Epoch 25/100\n",
      "293/293 [==============================] - 49s 167ms/step - loss: 0.2772 - mae: 0.3097 - mse: 0.2772 - val_loss: 0.2974 - val_mae: 0.2814 - val_mse: 0.2974\n",
      "Epoch 26/100\n",
      "293/293 [==============================] - 47s 162ms/step - loss: 0.2722 - mae: 0.3058 - mse: 0.2722 - val_loss: 0.2990 - val_mae: 0.2906 - val_mse: 0.2990\n",
      "Epoch 27/100\n",
      "293/293 [==============================] - 47s 162ms/step - loss: 0.2727 - mae: 0.3066 - mse: 0.2727 - val_loss: 0.3029 - val_mae: 0.3024 - val_mse: 0.3029\n",
      "Epoch 28/100\n",
      "293/293 [==============================] - 47s 161ms/step - loss: 0.2719 - mae: 0.3039 - mse: 0.2719 - val_loss: 0.3423 - val_mae: 0.3297 - val_mse: 0.3423\n",
      "Epoch 29/100\n",
      "293/293 [==============================] - 47s 162ms/step - loss: 0.2794 - mae: 0.3111 - mse: 0.2794 - val_loss: 0.3190 - val_mae: 0.3518 - val_mse: 0.3190\n",
      "Epoch 30/100\n",
      "293/293 [==============================] - 47s 162ms/step - loss: 0.2674 - mae: 0.3090 - mse: 0.2674 - val_loss: 0.3025 - val_mae: 0.2918 - val_mse: 0.3025\n",
      "Epoch 31/100\n",
      "293/293 [==============================] - 48s 162ms/step - loss: 0.2658 - mae: 0.3017 - mse: 0.2658 - val_loss: 0.3213 - val_mae: 0.3124 - val_mse: 0.3213\n",
      "Epoch 32/100\n",
      "293/293 [==============================] - 47s 162ms/step - loss: 0.2635 - mae: 0.3010 - mse: 0.2635 - val_loss: 0.3302 - val_mae: 0.3274 - val_mse: 0.3302\n",
      "Epoch 33/100\n",
      "293/293 [==============================] - 48s 163ms/step - loss: 0.2645 - mae: 0.2966 - mse: 0.2645 - val_loss: 0.3282 - val_mae: 0.3341 - val_mse: 0.3282\n",
      "Epoch 34/100\n",
      "293/293 [==============================] - 47s 162ms/step - loss: 0.2638 - mae: 0.3049 - mse: 0.2638 - val_loss: 0.3029 - val_mae: 0.2901 - val_mse: 0.3029\n",
      "Epoch 35/100\n",
      "293/293 [==============================] - 47s 161ms/step - loss: 0.2669 - mae: 0.3020 - mse: 0.2669 - val_loss: 0.3440 - val_mae: 0.3437 - val_mse: 0.3440\n",
      "finished training\n",
      "*------------------------------*\n",
      "\n",
      "starting evaluate\n",
      "*------------------------------*\n",
      "RMSE Score: 0.5453789268884527\n",
      "MAE Score: 0.5304499481845472\n",
      "finished evaluate\n",
      "*------------------------------*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_list = asset_df_B['full_path'].values\n",
    "target = asset_df_B['target'].values\n",
    "\n",
    "model_B = train(path_list, target, losses.mean_squared_error)\n",
    "# save_model(model_B, \"../models/baselineB.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c4e02b-f661-43bc-9ec9-ef0f4d627d73",
   "metadata": {},
   "source": [
    "## Evaluate model  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2302c3d7-1051-4c23-8717-aba374fd0d01",
   "metadata": {},
   "source": [
    "### Task A"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d163442f-e3aa-41b7-a28f-3f6e1ca1bbe4",
   "metadata": {},
   "source": [
    "file_name = \"../models/baselineA.pkl\"\n",
    "model = load_model(file_name)\n",
    "\n",
    "meta_features =\\\n",
    "    asset_df_A['collection.name'].unique().tolist() + ['num_sales']\n",
    "\n",
    "path_list = np.vstack(\n",
    "    (asset_df_A['full_path'].values.reshape(-1, 1),\n",
    "     asset_df_B['full_path'].values.reshape(-1, 1))\n",
    ").reshape(-1)\n",
    "meta_data = np.vstack(\n",
    "    (asset_df_A[meta_features].values.reshape(-1, len(meta_features)),\n",
    "     asset_df_B[meta_features].values.reshape(-1, len(meta_features)))\n",
    ")\n",
    "target = np.vstack(\n",
    "    (asset_df_A['target'].values.reshape(-1, 1),\n",
    "     asset_df_B['target'].values.reshape(-1, 1))\n",
    ").reshape(-1)\n",
    "\n",
    "train_path, val_path, train_meta, val_meta, train_y, val_y =\\\n",
    "    train_test_split(path_list, meta_data, target, test_size=0.1, random_state=6174)\n",
    "\n",
    "val_gen = FullPathDataLoader(path_list=val_path,\n",
    "                             meta_data=val_meta, target=val_y,\n",
    "                             batch_size=1, shuffle=False, is_train=False)\n",
    "\n",
    "model.evaluate(val_gen, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c61d60-b0ad-4ffa-8568-debe83ba19eb",
   "metadata": {},
   "source": [
    "### Task B"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76536ea4-0ac3-4bf7-aeed-9b9c9b219bd9",
   "metadata": {},
   "source": [
    "file_name = \"../models/baselineB.pkl\"\n",
    "model = load_model(file_name)\n",
    "\n",
    "path_list = asset_df_B['full_path'].values\n",
    "meta_data = asset_df_B[meta_features].values\n",
    "target = asset_df_B['target'].values\n",
    "\n",
    "train_path, val_path, train_meta, val_meta, train_y, val_y =\\\n",
    "    train_test_split(path_list, meta_data, target, test_size=0.1, random_state=6174)\n",
    "\n",
    "val_gen = FullPathDataLoader(path_list=val_path,\n",
    "                             meta_data=val_meta, target=val_y,\n",
    "                             batch_size=1, shuffle=False, is_train=False)\n",
    "\n",
    "model.evaluate(val_gen, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9d8f1-6140-4fb9-9369-f1cd005f0281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
