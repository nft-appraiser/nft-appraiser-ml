{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b83eaf3b-a15b-42fd-99da-dc9e6950f7bd",
   "metadata": {},
   "source": [
    "# モデル改善の試行錯誤用ノートブック  \n",
    "\n",
    "# 価格予測モデルのBaseline  \n",
    "- CNNを用いたモデルを作成する．  \n",
    "- 価格予測とクラス分類でタスクが大きく異なるので，imagenetで学習したモデルを用いないものを最初に作成する．  \n",
    "- サイトに載せられる画像を教師データとしており，画像が大きく回転したりなどは不要と考えられるためそのような前処理は行わない．  \n",
    "- 損失関数にはmaeもしくはrmseを用いる．  \n",
    "\n",
    "## モデルの構築  \n",
    "- EfficientNetB0（未学習）を用いて特徴量を抽出．  \n",
    "- num_sales, コレクション名のone-hotベクトルを抽出した特徴量に結合．  \n",
    "- 全結合層を重ねて出力．  \n",
    "- ImageNetを用いて事前学習したものとしていないもので比較する．  \n",
    "- 目的変数をそのまま予測するとスケールが大きすぎるので，先に対数変換して評価関数にRMSE, MAEなどを用いるほうが良いかも．  \n",
    "- **このノートブックでやっているのは事前学習有り．**  \n",
    "\n",
    "## 評価関数  \n",
    "- RMSLEを用いる．  \n",
    "$$RMSLE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (\\log{(y_i+1)} - \\log{(\\hat{y_i} +1)})^2}$$\n",
    "\n",
    "- 追加でMAPEを用いてみる．  \n",
    "$$MAPE = \\frac{100}{n} \\sum_{i=1}^n |\\frac{\\hat{y}_i - y_i}{y_i}|$$\n",
    "\n",
    "タスクAに関してはデータ不足の可能性が考えられるため，特徴量抽出とともにデータを追加で収集する．  \n",
    "\n",
    "## 変数（タスクA）  \n",
    "- 目的変数: last_sale.total_price  \n",
    "- 説明変数: 画像データ，コレクション名（collection.name），num_sales，\n",
    "\n",
    "## 変数（タスクB）  \n",
    "- 目的変数: last_sale.total_price  \n",
    "- 説明変数: 画像データ  \n",
    "\n",
    "### 実験結果  \n",
    "- 2021/11/16: ベースモデルをSwinTransformerにして学習\n",
    "    - SwinTransformerのGit: https://github.com/rishigami/Swin-Transformer-TF\n",
    "    - Apach Lisence 2.0のため商用利用可能  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd6e453-d7a2-447a-94c2-e3e41e7c841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "import math\n",
    "import tempfile\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.losses as losses\n",
    "import tensorflow.keras.optimizers as optim\n",
    "import tensorflow.keras.activations as activations\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "import tensorflow.keras.callbacks as callbacks\n",
    "from tensorflow.keras.applications import EfficientNetB0 as efn\n",
    "import cloudpickle\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from swintransformer import SwinTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa2369f-6c62-4ec4-a3cd-6790e8794ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (3,27,28,71,88,119) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (21747, 169)\n",
      "data shape: (7861, 169)\n"
     ]
    }
   ],
   "source": [
    "A_IMGPATH = \"../data/taskA/img\"\n",
    "A_DFPATH = \"../data/taskA/table\"\n",
    "B_IMGPATH = \"../data/taskB/img\"\n",
    "B_DFPATH = \"../data/taskB/table\"\n",
    "asset_df_A = pd.read_csv(os.path.join(A_DFPATH, \"asset_data.csv\"))\n",
    "asset_df_B = pd.read_csv(os.path.join(B_DFPATH, \"asset_data.csv\"))\n",
    "\n",
    "asset_df_A = asset_df_A.rename(columns={\"last_sale.total_price\": \"target\"})\n",
    "asset_df_B = asset_df_B.rename(columns={\"last_sale.total_price\": \"target\"})\n",
    "\n",
    "asset_df_A = pd.concat((asset_df_A, pd.get_dummies(asset_df_A[\"collection.name\"])), axis=1)\n",
    "asset_df_B[asset_df_A[\"collection.name\"].unique()] = 0\n",
    "\n",
    "asset_df_A[\"full_path\"] =\\\n",
    "    asset_df_A[\"image_id\"].apply(lambda x: A_IMGPATH + \"/\" + x)\n",
    "asset_df_B[\"full_path\"] =\\\n",
    "    asset_df_B[\"image_id\"].apply(lambda x: B_IMGPATH + \"/\" + x)\n",
    "\n",
    "asset_df_A['target'] = asset_df_A['target'].astype(float) * 1e-18\n",
    "asset_df_B['target'] = asset_df_B['target'].astype(float) * 1e-18\n",
    "asset_df_A = asset_df_A.query('target > 0')\n",
    "asset_df_B = asset_df_B.query('target > 0')\n",
    "asset_df_A['target'] = asset_df_A['target'].apply(lambda x: np.log1p(x))\n",
    "asset_df_B['target'] = asset_df_B['target'].apply(lambda x: np.log1p(x))\n",
    "\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "print(f\"data shape: {asset_df_A.shape}\")\n",
    "print(f\"data shape: {asset_df_B.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9128f-90a3-4a76-a850-c81273b9967d",
   "metadata": {},
   "source": [
    "## Helper Functions  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2126e4f-9af4-4b15-9bde-dc1a5f8a1124",
   "metadata": {},
   "source": [
    "### DataLoader  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb23666-1673-4db8-85da-9f8aa0fcbd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullPathDataLoader(Sequence):\n",
    "    \"\"\"\n",
    "    Data loader that load images, meta data and targets.\n",
    "    This class is inherited Sequence class of Keras.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path_list: np.ndarray, target: Optional[np.ndarray],\n",
    "                 meta_data: Optional[np.ndarray] = None, batch_size: int = 16,\n",
    "                 task: str = \"B\", width: int = 256, height: int = 256,\n",
    "                 resize: bool = True, shuffle: bool = True, is_train: bool = True):\n",
    "        \"\"\"\n",
    "        Constructor. This method determines class variables.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path_list : np.ndarray[str]\n",
    "            The array of absolute paths of images.\n",
    "        meta_data : np.ndarray[int]\n",
    "            One-hot vector of collections.\n",
    "        target : np.ndarray\n",
    "            Array of target variavles.\n",
    "        batch_size : int\n",
    "            Batch size used when model training.\n",
    "        task : str\n",
    "            Please determine this data loader will be used for task A or B(default=A).\n",
    "        width : int\n",
    "            Width of resized image.\n",
    "        height : int\n",
    "            Height of resize image.\n",
    "        resize : bool\n",
    "            Flag determine whether to resize.\n",
    "        shuffle : bool\n",
    "            Flag determine whether to shuffle on epoch end.\n",
    "        is_train : bool\n",
    "            Determine whether this data loader will be used training model.\n",
    "            if you won't this data loader, you have set 'is_train'=False.\n",
    "        \"\"\"\n",
    "        self.path_list = path_list\n",
    "        self.batch_size = batch_size\n",
    "        self.task = task\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.resize = resize\n",
    "        self.shuffle = shuffle\n",
    "        self.is_train = is_train\n",
    "        self.length = math.ceil(len(self.path_list) / self.batch_size)\n",
    "\n",
    "        if self.is_train:\n",
    "            self.target = target\n",
    "        if self.task == \"A\":\n",
    "            self.meta_data = meta_data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        self.length : data length\n",
    "        \"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def get_img(self, path_list: np.ndarray):\n",
    "        \"\"\"\n",
    "        Load image data and resize image if 'resize'=True.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path_liist : np.ndarray\n",
    "            The array of relative image paths from directory 'dir_name'.\n",
    "            Size of this array is 'batch_size'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        img_list : np.ndarray\n",
    "            The array of image data.\n",
    "            Size of an image is (width, height, 3) if 'resize'=True.\n",
    "        '\"\"\"\n",
    "        img_list = []\n",
    "        for path in path_list:\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.resize(img, (self.width, self.height))\n",
    "            img = img / 255.\n",
    "            img_list.append(img)\n",
    "\n",
    "        img_list = np.array(img_list)\n",
    "        return img_list\n",
    "\n",
    "    def _shuffle(self):\n",
    "        \"\"\"\n",
    "        Shuffle path_list, meta model.\n",
    "        If 'is_train' is True, target is shuffled in association path_list.\n",
    "        \"\"\"\n",
    "        idx = np.random.permutation(len(self.path_list))\n",
    "        self.path_list = self.path_list[idx]\n",
    "        if self.task == \"A\":\n",
    "            self.meta_data = self.meta_data[idx]\n",
    "        if self.is_train:\n",
    "            self.target = self.target[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_list = self.path_list[self.batch_size*idx:self.batch_size*(idx+1)]\n",
    "        img_list = self.get_img(path_list)\n",
    "        if self.is_train:\n",
    "            target_list = self.target[self.batch_size*idx:self.batch_size*(idx+1)]\n",
    "            if self.task == \"A\":\n",
    "                meta = self.meta_data[self.batch_size*idx:self.batch_size*(idx+1)]\n",
    "                return (img_list, meta), target_list\n",
    "            else:\n",
    "                return img_list, target_list\n",
    "        else:\n",
    "            if self.task == \"A\":\n",
    "                meta = self.meta_data[self.batch_size*idx:self.batch_size*(idx+1)]\n",
    "                return ((img_list, meta),)\n",
    "            else:\n",
    "                return img_list\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.is_train:\n",
    "            self._shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccabc2f6-749f-404a-a617-9f0507113d78",
   "metadata": {},
   "source": [
    "### seed settings  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44fab9da-4ce3-4548-ba52-29baf9c5e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(random_state=6174):\n",
    "    tf.random.set_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "    random.seed(random_state)\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab18eb9-6e4f-40f8-9ffc-c2b394b47870",
   "metadata": {},
   "source": [
    "### Create model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae81ae9f-aead-43ec-9c7b-906a40574366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape: Tuple[int], output_shape: int,\n",
    "                 activation, loss, meta_shape: Optional[int] = None,\n",
    "                 task: str = \"B\", learning_rate: float = 0.001,\n",
    "                 pretrain: bool = False) -> models.Model:\n",
    "    \"\"\"\n",
    "    The function for creating model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : int\n",
    "        Shape of input image data.\n",
    "    output_shape : int\n",
    "        Shape of model output.\n",
    "    activation : function\n",
    "        The activation function used hidden layers.\n",
    "    loss : function\n",
    "        The loss function of model.\n",
    "    meta_shape : int\n",
    "        Shape of input meta data of image.\n",
    "    task : str\n",
    "        Please determine this model will be used for task A or B(default=A).\n",
    "    learning_rate : float\n",
    "        The learning rate of model.\n",
    "    pretrain : bool\n",
    "        Flag that deterimine whether use pretrain model(default=False).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : keras.models.Model\n",
    "        Model instance.\n",
    "    \"\"\"\n",
    "    if pretrain:\n",
    "        weights = 'imagenet'\n",
    "    else:\n",
    "        weights = None\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    base_model = SwinTransformer('swin_tiny_224', include_top=False, pretrained=True, use_tpu=False)(inputs)\n",
    "\n",
    "    if task == \"A\":\n",
    "        meta_inputs = layers.Input(shape=meta_shape)\n",
    "        concate = layers.Concatenate()([base_model, meta_inputs])\n",
    "        dense1 = layers.Dense(units=128)(concate)\n",
    "        av1 = layers.Activation(activation)(dense1)\n",
    "        dr1 = layers.Dropout(0.3)(av1)\n",
    "        dense2 = layers.Dense(units=64)(dr1)\n",
    "        av2 = layers.Activation(activation)(dense2)\n",
    "        dr2 = layers.Dropout(0.3)(av2)\n",
    "        outputs = layers.Dense(output_shape)(dr2)\n",
    "\n",
    "        model = models.Model(inputs=[inputs, meta_inputs], outputs=[outputs])\n",
    "\n",
    "    elif task == \"B\":\n",
    "        dense1 = layers.Dense(units=128)(base_model)\n",
    "        av1 = layers.Activation(activation)(dense1)\n",
    "        dr1 = layers.Dropout(0.3)(av1)\n",
    "        dense2 = layers.Dense(units=64)(dr1)\n",
    "        av2 = layers.Activation(activation)(dense2)\n",
    "        dr2 = layers.Dropout(0.3)(av2)\n",
    "        outputs = layers.Dense(output_shape)(dr2)\n",
    "\n",
    "        model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Please set task is A or B.\")\n",
    "\n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=optim.Adam(learning_rate=learning_rate),\n",
    "                  metrics=['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d8ef5c-83cf-4e06-a76a-ebac170eab62",
   "metadata": {},
   "source": [
    "### Training model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d69c795d-f36c-45e1-88b4-f3c591e4f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(path_list: np.ndarray, target: np.ndarray, loss,\n",
    "          meta_data: Optional[np.ndarray] = None, task: str = \"B\"):\n",
    "    \"\"\"\n",
    "    The function for training model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_list : np.ndarray\n",
    "        The path list of all image data.\n",
    "    target : np.ndarray\n",
    "        The array of targets data.\n",
    "    loss : function\n",
    "        The loss function of keras.\n",
    "    meta_data : np.ndarray\n",
    "        The array of meta data of image.\n",
    "    task : str\n",
    "        Please determine you train model for task A or B(default=A).\n",
    "    \"\"\"\n",
    "    if task == \"A\":\n",
    "        train_path, val_path, train_meta, val_meta, train_y, val_y =\\\n",
    "            train_test_split(path_list, meta_data, target, test_size=0.1, random_state=6174)\n",
    "        train_gen = FullPathDataLoader(path_list=train_path, target=train_y,\n",
    "                                       meta_data=train_meta, batch_size=16,\n",
    "                                       width=224, height=224, task=task)\n",
    "        val_gen = FullPathDataLoader(path_list=val_path, target=train_y,\n",
    "                                     meta_data=val_meta, batch_size=1,\n",
    "                                     width=224, height=224, task=task)\n",
    "    elif task == \"B\":\n",
    "        train_path, val_path, train_y, val_y =\\\n",
    "            train_test_split(path_list, target, test_size=0.1, random_state=6174)\n",
    "        train_gen = FullPathDataLoader(path_list=train_path, target=train_y,\n",
    "                                       width=224, height=224, batch_size=16, task=task)\n",
    "        val_gen = FullPathDataLoader(path_list=val_path, target=val_y,\n",
    "                                     width=224, height=224, batch_size=1, task=task)\n",
    "    else:\n",
    "        raise Exception(\"Please set task is A or B\")\n",
    "\n",
    "    set_seed()\n",
    "    model = NFTModel(\n",
    "        create_model(input_shape=(224, 224, 3), output_shape=1,\n",
    "                     activation=activations.relu, loss=loss,\n",
    "                     meta_shape=len(meta_features), task=task,\n",
    "                     learning_rate=0.00001, pretrain=True)\n",
    "    )\n",
    "\n",
    "    ES = callbacks.EarlyStopping(monitor='val_loss', patience=10,\n",
    "                                 restore_best_weights=True)\n",
    "\n",
    "    print(\"starting training\")\n",
    "    print('*' + '-' * 30 + '*')\n",
    "\n",
    "    model.fit(train_gen, val_gen, epochs=100, batch_size=16,\n",
    "              callbacks=[ES])\n",
    "\n",
    "    print(\"finished training\")\n",
    "    print('*' + '-' * 30 + '*' + '\\n')\n",
    "\n",
    "    if task == \"A\":\n",
    "        val_gen = FullPathDataLoader(path_list=val_path, target=train_y,\n",
    "                                     meta_data=val_meta, batch_size=1, task=task,\n",
    "                                     width=224, height=224, shuffle=False, is_train=False)\n",
    "    else:\n",
    "        val_gen = FullPathDataLoader(path_list=val_path, target=train_y,\n",
    "                                     batch_size=1, task=task,\n",
    "                                     width=224, height=224, shuffle=False, is_train=False)\n",
    "    print(\"starting evaluate\")\n",
    "    print('*' + '-' * 30 + '*')\n",
    "\n",
    "    model.evaluate(val_gen, val_y)\n",
    "\n",
    "    print(\"finished evaluate\")\n",
    "    print('*' + '-' * 30 + '*' + '\\n')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8611a18a-51e3-4841-ac3b-3b3400d8ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFTModel(KerasRegressor):\n",
    "    \"\"\"\n",
    "    Model class.\n",
    "    This class is inherited KerasRegressor class of keras.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_func):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        Prameters\n",
    "        ---------\n",
    "        model_func : function\n",
    "            The function for creating model.\n",
    "        \"\"\"\n",
    "        super().__init__(build_fn=model_func)\n",
    "        self.model = self.build_fn\n",
    "\n",
    "    def __getstate__(self):\n",
    "        result = {'sk_params': self.sk_params}\n",
    "        with tempfile.TemporaryDirectory() as dir:\n",
    "            if hasattr(self, 'model'):\n",
    "                self.model.save(dir + '/output.h5', include_optimizer=False)\n",
    "                with open(dir + '/output.h5', 'rb') as f:\n",
    "                    result['model'] = f.read()\n",
    "        return result\n",
    "\n",
    "    def __setstate__(self, serialized):\n",
    "        self.sk_params = serialized['sk_params']\n",
    "        with tempfile.TemporaryDirectory() as dir:\n",
    "            model_data = serialized.get('model')\n",
    "            if model_data:\n",
    "                with open(dir + '/input.h5', 'wb') as f:\n",
    "                    f.write(model_data)\n",
    "                self.model = tf.keras.models.load_model(dir + '/input.h5')\n",
    "\n",
    "    def fit(self, train_gen, val_gen, epochs, batch_size, callbacks=None):\n",
    "        \"\"\"\n",
    "        Training model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_gen : iterator\n",
    "            The generator of train data.\n",
    "        val_gen : iterator\n",
    "            The generator of validation data.\n",
    "        epochs : int\n",
    "            Number of epochs for training model.\n",
    "        batch_size : int\n",
    "            Size of batch for training model.\n",
    "        callbacks : list\n",
    "            The list of callbacks.\n",
    "            For example [EarlyStopping instance, ModelCheckpoint instance]\n",
    "        \"\"\"\n",
    "        self.model.fit(train_gen, epochs=epochs, batch_size=batch_size,\n",
    "                       validation_data=val_gen, callbacks=callbacks)\n",
    "\n",
    "    def evaluate(self, test_X, test_y):\n",
    "        \"\"\"\n",
    "        Evaluate model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        test_X : iterator\n",
    "            The generator of test data.\n",
    "        test_y : np.ndarray\n",
    "            The array of targets of test data.\n",
    "        \"\"\"\n",
    "        pred = self.model.predict(test_X)\n",
    "        pred = np.where(pred < 0, 0, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(test_y, pred))\n",
    "        mae = np.sqrt(mean_absolute_error(test_y, pred))\n",
    "\n",
    "        print(f\"RMSE Score: {rmse}\")\n",
    "        print(f\"MAE Score: {mae}\")\n",
    "\n",
    "    def predict(self, img_path: str, collection_name: str, num_sales: int,\n",
    "                task: str = \"B\"):\n",
    "        \"\"\"\n",
    "        Predict data using trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img_path : str\n",
    "            The path of image data.\n",
    "        collection_name : str\n",
    "            Name of collection of the NFT.\n",
    "        num_sales : int\n",
    "            Number of times the NFT sold.\n",
    "        \"\"\"\n",
    "        if task == \"A\":\n",
    "            collections = ['CryptoPunks',\n",
    "                           'Bored Ape Yacht Club',\n",
    "                           'Edifice by Ben Kovach',\n",
    "                           'Mutant Ape Yacht Club',\n",
    "                           'The Sandbox',\n",
    "                           'Divine Anarchy',\n",
    "                           'Cosmic Labs',\n",
    "                           'Parallel Alpha',\n",
    "                           'Art Wars | AW',\n",
    "                           'Neo Tokyo Identities',\n",
    "                           'Neo Tokyo Part 2 Vault Cards',\n",
    "                           'Cool Cats NFT',\n",
    "                           'CrypToadz by GREMPLIN',\n",
    "                           'BearXLabs',\n",
    "                           'Desperate ApeWives',\n",
    "                           'Decentraland',\n",
    "                           'Neo Tokyo Part 3 Item Caches',\n",
    "                           'Doodles',\n",
    "                           'The Doge Pound',\n",
    "                           'Playboy Rabbitars Official',\n",
    "                           'THE SHIBOSHIS',\n",
    "                           'THE REAL GOAT SOCIETY',\n",
    "                           'Sipherian Flash',\n",
    "                           'Party Ape | Billionaire Club',\n",
    "                           'Treeverse',\n",
    "                           'Angry Apes United',\n",
    "                           'CyberKongz',\n",
    "                           'Emblem Vault [Ethereum]',\n",
    "                           'Fat Ape Club',\n",
    "                           'VeeFriends',\n",
    "                           'JUNGLE FREAKS BY TROSLEY',\n",
    "                           'Meebits',\n",
    "                           'Furballs.com Official',\n",
    "                           'Kaiju Kingz',\n",
    "                           'Bears Deluxe',\n",
    "                           'PUNKS Comic',\n",
    "                           'Hor1zon Troopers',\n",
    "                           'Lazy Lions',\n",
    "                           'LOSTPOETS',\n",
    "                           'Chain Runners',\n",
    "                           'Chromie Squiggle by Snowfro',\n",
    "                           'MekaVerse',\n",
    "                           'Vox Collectibles',\n",
    "                           'MutantCats',\n",
    "                           'World of Women',\n",
    "                           'SuperFarm Genesis Series',\n",
    "                           'Eponym by ART AI',]\n",
    "            collection_dict = {\n",
    "                 collections[i]: i for i in range(len(collections))\n",
    "            }\n",
    "            meta_data = np.zeros(shape=(len(collection_dict)+1))\n",
    "            if collection_name in collection_dict.keys():\n",
    "                meta_data[collection_dict[collection_name]] = 1\n",
    "            meta_data[-1] = num_sales\n",
    "            meta_data = meta_data.reshape(1, -1)\n",
    "\n",
    "            img = cv2.resize(cv2.imread(img_path)/255., (224, 224))\n",
    "            img = img.reshape(1, 224, 224, 3)\n",
    "\n",
    "            pred = self.model.predict([img, meta_data])\n",
    "        elif task == \"B\":\n",
    "            img = cv2.resize(cv2.imread(img_path)/255., (224, 224))\n",
    "            img = img.reshape(1, 224, 224, 3)\n",
    "\n",
    "            pred = self.model.predict(img)\n",
    "        else:\n",
    "            raise Exception(\"Please set task is A or B\")\n",
    "\n",
    "        return pred[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "184a4688-00bf-47b8-8e57-635cb9f0f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(instance, file_name: str):\n",
    "    \"\"\"\n",
    "    Save model as pickle file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    instance : Class instance\n",
    "        The class instance you want to save as pickle file.\n",
    "    file_name : str\n",
    "        The absolute path of file saved the instance.\n",
    "    \"\"\"\n",
    "    with open(file_name, mode='wb') as f:\n",
    "        cloudpickle.dump(instance, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9261d8b-47c3-4e1c-8bfa-f98763d74a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file_name: str):\n",
    "    \"\"\"\n",
    "    Load the model file of pickle.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        The absolute path of the model file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : tf.keras.models.Model\n",
    "        Trained model object.\n",
    "    \"\"\"\n",
    "    with open(file_name, mode='rb') as f:\n",
    "        model = cloudpickle.load(f)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b4cb72-e675-4a37-93d2-c76038d38b58",
   "metadata": {},
   "source": [
    "## Training models  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4e9dc9-b1eb-462a-bd8c-a8b559d24e0d",
   "metadata": {},
   "source": [
    "### TaskA（画像のみ）  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dcb07da4-d972-487b-90c1-a7d45e057d1f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "*------------------------------*\n",
      "Epoch 1/100\n",
      "1224/1224 [==============================] - 343s 274ms/step - loss: 1.1803 - mae: 0.6198 - mse: 1.1803 - val_loss: 0.7629 - val_mae: 0.4420 - val_mse: 0.7629\n",
      "Epoch 2/100\n",
      "1224/1224 [==============================] - 327s 267ms/step - loss: 0.9171 - mae: 0.4946 - mse: 0.9171 - val_loss: 0.6759 - val_mae: 0.3442 - val_mse: 0.6759\n",
      "Epoch 3/100\n",
      "1224/1224 [==============================] - 326s 267ms/step - loss: 0.8033 - mae: 0.4467 - mse: 0.8033 - val_loss: 0.6237 - val_mae: 0.3229 - val_mse: 0.6237\n",
      "Epoch 4/100\n",
      "1224/1224 [==============================] - 326s 266ms/step - loss: 0.7471 - mae: 0.4212 - mse: 0.7471 - val_loss: 0.6529 - val_mae: 0.2946 - val_mse: 0.6529\n",
      "Epoch 5/100\n",
      "1224/1224 [==============================] - 326s 266ms/step - loss: 0.7228 - mae: 0.4025 - mse: 0.7228 - val_loss: 0.5931 - val_mae: 0.2992 - val_mse: 0.5931\n",
      "Epoch 6/100\n",
      "1224/1224 [==============================] - 321s 263ms/step - loss: 0.6643 - mae: 0.3886 - mse: 0.6643 - val_loss: 0.5740 - val_mae: 0.2854 - val_mse: 0.5740\n",
      "Epoch 7/100\n",
      "1224/1224 [==============================] - 322s 263ms/step - loss: 0.6240 - mae: 0.3731 - mse: 0.6240 - val_loss: 0.5618 - val_mae: 0.2770 - val_mse: 0.5618\n",
      "Epoch 8/100\n",
      "1224/1224 [==============================] - 322s 263ms/step - loss: 0.5598 - mae: 0.3630 - mse: 0.5598 - val_loss: 0.6481 - val_mae: 0.3242 - val_mse: 0.6481\n",
      "Epoch 9/100\n",
      "1224/1224 [==============================] - 322s 263ms/step - loss: 0.5311 - mae: 0.3495 - mse: 0.5311 - val_loss: 0.7521 - val_mae: 0.3065 - val_mse: 0.7521\n",
      "Epoch 10/100\n",
      "1224/1224 [==============================] - 319s 261ms/step - loss: 0.4930 - mae: 0.3408 - mse: 0.4930 - val_loss: 0.6224 - val_mae: 0.3221 - val_mse: 0.6224\n",
      "Epoch 11/100\n",
      "1224/1224 [==============================] - 313s 256ms/step - loss: 0.4378 - mae: 0.3248 - mse: 0.4378 - val_loss: 0.7164 - val_mae: 0.2943 - val_mse: 0.7164\n",
      "Epoch 12/100\n",
      "1224/1224 [==============================] - 313s 255ms/step - loss: 0.4173 - mae: 0.3244 - mse: 0.4173 - val_loss: 0.5457 - val_mae: 0.2684 - val_mse: 0.5457\n",
      "Epoch 13/100\n",
      "1224/1224 [==============================] - 315s 257ms/step - loss: 0.3685 - mae: 0.3146 - mse: 0.3685 - val_loss: 0.8344 - val_mae: 0.3374 - val_mse: 0.8344\n",
      "Epoch 14/100\n",
      "1224/1224 [==============================] - 318s 260ms/step - loss: 0.4109 - mae: 0.3120 - mse: 0.4109 - val_loss: 0.6492 - val_mae: 0.2985 - val_mse: 0.6492\n",
      "Epoch 15/100\n",
      "1224/1224 [==============================] - 314s 257ms/step - loss: 0.3503 - mae: 0.3040 - mse: 0.3503 - val_loss: 0.8832 - val_mae: 0.2920 - val_mse: 0.8832\n",
      "Epoch 16/100\n",
      "1224/1224 [==============================] - 315s 257ms/step - loss: 0.3082 - mae: 0.3016 - mse: 0.3082 - val_loss: 0.7432 - val_mae: 0.3167 - val_mse: 0.7432\n",
      "Epoch 17/100\n",
      "1224/1224 [==============================] - 316s 258ms/step - loss: 0.3179 - mae: 0.2951 - mse: 0.3179 - val_loss: 0.8262 - val_mae: 0.2890 - val_mse: 0.8262\n",
      "Epoch 18/100\n",
      "1224/1224 [==============================] - 314s 256ms/step - loss: 0.2906 - mae: 0.2901 - mse: 0.2906 - val_loss: 0.6481 - val_mae: 0.2625 - val_mse: 0.6481\n",
      "Epoch 19/100\n",
      "1224/1224 [==============================] - 323s 264ms/step - loss: 0.2725 - mae: 0.2884 - mse: 0.2725 - val_loss: 0.6771 - val_mae: 0.2793 - val_mse: 0.6771\n",
      "Epoch 20/100\n",
      "1224/1224 [==============================] - 329s 269ms/step - loss: 0.2578 - mae: 0.2788 - mse: 0.2578 - val_loss: 0.8641 - val_mae: 0.2789 - val_mse: 0.8641\n",
      "Epoch 21/100\n",
      "1224/1224 [==============================] - 326s 267ms/step - loss: 0.2387 - mae: 0.2789 - mse: 0.2387 - val_loss: 0.6791 - val_mae: 0.2687 - val_mse: 0.6791\n",
      "Epoch 22/100\n",
      "1224/1224 [==============================] - 332s 271ms/step - loss: 0.2882 - mae: 0.2816 - mse: 0.2882 - val_loss: 0.8415 - val_mae: 0.2944 - val_mse: 0.8415\n",
      "finished training\n",
      "*------------------------------*\n",
      "\n",
      "starting evaluate\n",
      "*------------------------------*\n",
      "RMSE Score: 0.7387402257545599\n",
      "MAE Score: 0.5180658991716389\n",
      "finished evaluate\n",
      "*------------------------------*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_list = asset_df_A['full_path'].values\n",
    "target = asset_df_A['target'].values\n",
    "\n",
    "model_A = train(path_list, target, losses.mean_squared_error,\n",
    "                task=\"B\")\n",
    "# save_model(model_A, \"../models/swintransformerA.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87444d2c-ae2a-4cc6-9776-6ab420b16ed4",
   "metadata": {},
   "source": [
    "### TaskB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "abcc6969-e9b1-4aed-bde1-22975d18fb10",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "*------------------------------*\n",
      "Epoch 1/100\n",
      "443/443 [==============================] - 128s 271ms/step - loss: 0.2844 - mae: 0.2901 - mse: 0.2844 - val_loss: 0.1280 - val_mae: 0.2039 - val_mse: 0.1280\n",
      "Epoch 2/100\n",
      "443/443 [==============================] - 118s 266ms/step - loss: 0.2144 - mae: 0.2435 - mse: 0.2144 - val_loss: 0.1205 - val_mae: 0.1979 - val_mse: 0.1205\n",
      "Epoch 3/100\n",
      "443/443 [==============================] - 118s 266ms/step - loss: 0.1660 - mae: 0.2246 - mse: 0.1660 - val_loss: 0.1435 - val_mae: 0.2034 - val_mse: 0.1435\n",
      "Epoch 4/100\n",
      "443/443 [==============================] - 118s 266ms/step - loss: 0.1648 - mae: 0.2162 - mse: 0.1648 - val_loss: 0.1128 - val_mae: 0.1845 - val_mse: 0.1128\n",
      "Epoch 5/100\n",
      "443/443 [==============================] - 118s 266ms/step - loss: 0.1687 - mae: 0.2129 - mse: 0.1687 - val_loss: 0.1101 - val_mae: 0.1715 - val_mse: 0.1101\n",
      "Epoch 6/100\n",
      "443/443 [==============================] - 118s 266ms/step - loss: 0.1211 - mae: 0.1987 - mse: 0.1211 - val_loss: 0.0955 - val_mae: 0.1625 - val_mse: 0.0955\n",
      "Epoch 7/100\n",
      "443/443 [==============================] - 113s 256ms/step - loss: 0.1494 - mae: 0.1965 - mse: 0.1494 - val_loss: 0.0930 - val_mae: 0.1731 - val_mse: 0.0930\n",
      "Epoch 8/100\n",
      "443/443 [==============================] - 117s 265ms/step - loss: 0.1061 - mae: 0.1837 - mse: 0.1061 - val_loss: 0.0950 - val_mae: 0.1592 - val_mse: 0.0950\n",
      "Epoch 9/100\n",
      "443/443 [==============================] - 117s 264ms/step - loss: 0.1036 - mae: 0.1792 - mse: 0.1036 - val_loss: 0.0852 - val_mae: 0.1567 - val_mse: 0.0852\n",
      "Epoch 10/100\n",
      "443/443 [==============================] - 117s 265ms/step - loss: 0.0904 - mae: 0.1738 - mse: 0.0904 - val_loss: 0.0830 - val_mae: 0.1531 - val_mse: 0.0830\n",
      "Epoch 11/100\n",
      "443/443 [==============================] - 117s 264ms/step - loss: 0.0930 - mae: 0.1683 - mse: 0.0930 - val_loss: 0.0877 - val_mae: 0.1546 - val_mse: 0.0877\n",
      "Epoch 12/100\n",
      "443/443 [==============================] - 117s 265ms/step - loss: 0.0770 - mae: 0.1633 - mse: 0.0770 - val_loss: 0.0768 - val_mae: 0.1431 - val_mse: 0.0768\n",
      "Epoch 13/100\n",
      "443/443 [==============================] - 117s 264ms/step - loss: 0.0759 - mae: 0.1607 - mse: 0.0759 - val_loss: 0.0734 - val_mae: 0.1427 - val_mse: 0.0734\n",
      "Epoch 14/100\n",
      "443/443 [==============================] - 117s 264ms/step - loss: 0.0747 - mae: 0.1594 - mse: 0.0747 - val_loss: 0.0671 - val_mae: 0.1400 - val_mse: 0.0671\n",
      "Epoch 15/100\n",
      "443/443 [==============================] - 117s 264ms/step - loss: 0.0669 - mae: 0.1490 - mse: 0.0669 - val_loss: 0.0848 - val_mae: 0.1441 - val_mse: 0.0848\n",
      "Epoch 16/100\n",
      "443/443 [==============================] - 117s 265ms/step - loss: 0.0661 - mae: 0.1498 - mse: 0.0661 - val_loss: 0.0751 - val_mae: 0.1387 - val_mse: 0.0751\n",
      "Epoch 17/100\n",
      "443/443 [==============================] - 118s 265ms/step - loss: 0.0606 - mae: 0.1432 - mse: 0.0606 - val_loss: 0.0859 - val_mae: 0.1396 - val_mse: 0.0859\n",
      "Epoch 18/100\n",
      "443/443 [==============================] - 118s 266ms/step - loss: 0.0572 - mae: 0.1410 - mse: 0.0572 - val_loss: 0.0625 - val_mae: 0.1310 - val_mse: 0.0625\n",
      "Epoch 19/100\n",
      "443/443 [==============================] - 121s 273ms/step - loss: 0.0598 - mae: 0.1400 - mse: 0.0598 - val_loss: 0.0624 - val_mae: 0.1283 - val_mse: 0.0624\n",
      "Epoch 20/100\n",
      "443/443 [==============================] - 121s 274ms/step - loss: 0.0554 - mae: 0.1367 - mse: 0.0554 - val_loss: 0.0651 - val_mae: 0.1393 - val_mse: 0.0651\n",
      "Epoch 21/100\n",
      "443/443 [==============================] - 119s 269ms/step - loss: 0.0556 - mae: 0.1353 - mse: 0.0556 - val_loss: 0.0598 - val_mae: 0.1287 - val_mse: 0.0598\n",
      "Epoch 22/100\n",
      "443/443 [==============================] - 122s 276ms/step - loss: 0.0455 - mae: 0.1296 - mse: 0.0455 - val_loss: 0.0726 - val_mae: 0.1297 - val_mse: 0.0726\n",
      "Epoch 23/100\n",
      "443/443 [==============================] - 121s 272ms/step - loss: 0.0576 - mae: 0.1310 - mse: 0.0576 - val_loss: 0.0682 - val_mae: 0.1289 - val_mse: 0.0682\n",
      "Epoch 24/100\n",
      "443/443 [==============================] - 121s 273ms/step - loss: 0.0520 - mae: 0.1337 - mse: 0.0520 - val_loss: 0.0683 - val_mae: 0.1266 - val_mse: 0.0683\n",
      "Epoch 25/100\n",
      "443/443 [==============================] - 119s 268ms/step - loss: 0.0555 - mae: 0.1304 - mse: 0.0555 - val_loss: 0.0759 - val_mae: 0.1279 - val_mse: 0.0759\n",
      "Epoch 26/100\n",
      "443/443 [==============================] - 119s 269ms/step - loss: 0.0500 - mae: 0.1264 - mse: 0.0500 - val_loss: 0.0560 - val_mae: 0.1230 - val_mse: 0.0560\n",
      "Epoch 27/100\n",
      "443/443 [==============================] - 119s 269ms/step - loss: 0.0476 - mae: 0.1243 - mse: 0.0476 - val_loss: 0.0582 - val_mae: 0.1243 - val_mse: 0.0582\n",
      "Epoch 28/100\n",
      "443/443 [==============================] - 119s 269ms/step - loss: 0.0442 - mae: 0.1210 - mse: 0.0442 - val_loss: 0.0601 - val_mae: 0.1227 - val_mse: 0.0601\n",
      "Epoch 29/100\n",
      "443/443 [==============================] - 119s 269ms/step - loss: 0.0427 - mae: 0.1175 - mse: 0.0427 - val_loss: 0.0589 - val_mae: 0.1209 - val_mse: 0.0589\n",
      "Epoch 30/100\n",
      "443/443 [==============================] - 119s 269ms/step - loss: 0.0393 - mae: 0.1182 - mse: 0.0393 - val_loss: 0.0575 - val_mae: 0.1211 - val_mse: 0.0575\n",
      "Epoch 31/100\n",
      "443/443 [==============================] - 118s 267ms/step - loss: 0.0383 - mae: 0.1154 - mse: 0.0383 - val_loss: 0.0524 - val_mae: 0.1224 - val_mse: 0.0524\n",
      "Epoch 32/100\n",
      "443/443 [==============================] - 120s 271ms/step - loss: 0.0485 - mae: 0.1193 - mse: 0.0485 - val_loss: 0.0747 - val_mae: 0.1251 - val_mse: 0.0747\n",
      "Epoch 33/100\n",
      "443/443 [==============================] - 117s 264ms/step - loss: 0.0390 - mae: 0.1141 - mse: 0.0390 - val_loss: 0.0580 - val_mae: 0.1201 - val_mse: 0.0580\n",
      "Epoch 34/100\n",
      "443/443 [==============================] - 117s 265ms/step - loss: 0.0453 - mae: 0.1130 - mse: 0.0453 - val_loss: 0.0535 - val_mae: 0.1162 - val_mse: 0.0535\n",
      "Epoch 35/100\n",
      "443/443 [==============================] - 120s 270ms/step - loss: 0.0459 - mae: 0.1124 - mse: 0.0459 - val_loss: 0.0604 - val_mae: 0.1159 - val_mse: 0.0604\n",
      "Epoch 36/100\n",
      "443/443 [==============================] - 304s 688ms/step - loss: 0.0482 - mae: 0.1140 - mse: 0.0482 - val_loss: 0.0636 - val_mae: 0.1236 - val_mse: 0.0636\n",
      "Epoch 37/100\n",
      "443/443 [==============================] - 351s 792ms/step - loss: 0.0401 - mae: 0.1102 - mse: 0.0401 - val_loss: 0.0547 - val_mae: 0.1159 - val_mse: 0.0547\n",
      "Epoch 38/100\n",
      "443/443 [==============================] - 349s 789ms/step - loss: 0.0320 - mae: 0.1057 - mse: 0.0320 - val_loss: 0.0663 - val_mae: 0.1152 - val_mse: 0.0663\n",
      "Epoch 39/100\n",
      "443/443 [==============================] - 214s 482ms/step - loss: 0.0331 - mae: 0.1059 - mse: 0.0331 - val_loss: 0.0562 - val_mae: 0.1124 - val_mse: 0.0562\n",
      "Epoch 40/100\n",
      "443/443 [==============================] - 118s 267ms/step - loss: 0.0369 - mae: 0.1069 - mse: 0.0369 - val_loss: 0.0668 - val_mae: 0.1122 - val_mse: 0.0668\n",
      "Epoch 41/100\n",
      "443/443 [==============================] - 119s 268ms/step - loss: 0.0355 - mae: 0.1066 - mse: 0.0355 - val_loss: 0.0513 - val_mae: 0.1218 - val_mse: 0.0513\n",
      "Epoch 42/100\n",
      "443/443 [==============================] - 119s 268ms/step - loss: 0.0383 - mae: 0.1055 - mse: 0.0383 - val_loss: 0.0501 - val_mae: 0.1109 - val_mse: 0.0501\n",
      "Epoch 43/100\n",
      "443/443 [==============================] - 118s 267ms/step - loss: 0.0303 - mae: 0.1020 - mse: 0.0303 - val_loss: 0.0523 - val_mae: 0.1139 - val_mse: 0.0523\n",
      "Epoch 44/100\n",
      "443/443 [==============================] - 119s 269ms/step - loss: 0.0376 - mae: 0.1033 - mse: 0.0376 - val_loss: 0.0502 - val_mae: 0.1160 - val_mse: 0.0502\n",
      "Epoch 45/100\n",
      "443/443 [==============================] - 119s 268ms/step - loss: 0.0347 - mae: 0.1033 - mse: 0.0347 - val_loss: 0.0581 - val_mae: 0.1155 - val_mse: 0.0581\n",
      "Epoch 46/100\n",
      "443/443 [==============================] - 119s 268ms/step - loss: 0.0403 - mae: 0.1034 - mse: 0.0403 - val_loss: 0.0480 - val_mae: 0.1074 - val_mse: 0.0480\n",
      "Epoch 47/100\n",
      "443/443 [==============================] - 119s 268ms/step - loss: 0.0313 - mae: 0.1013 - mse: 0.0313 - val_loss: 0.0500 - val_mae: 0.1076 - val_mse: 0.0500\n",
      "Epoch 48/100\n",
      "443/443 [==============================] - 119s 269ms/step - loss: 0.0329 - mae: 0.0994 - mse: 0.0329 - val_loss: 0.0535 - val_mae: 0.1162 - val_mse: 0.0535\n",
      "Epoch 49/100\n",
      "443/443 [==============================] - 119s 268ms/step - loss: 0.0318 - mae: 0.0998 - mse: 0.0318 - val_loss: 0.0514 - val_mae: 0.1113 - val_mse: 0.0514\n",
      "Epoch 50/100\n",
      "443/443 [==============================] - 119s 268ms/step - loss: 0.0281 - mae: 0.0982 - mse: 0.0281 - val_loss: 0.0561 - val_mae: 0.1171 - val_mse: 0.0561\n",
      "Epoch 51/100\n",
      "443/443 [==============================] - 119s 268ms/step - loss: 0.0303 - mae: 0.0967 - mse: 0.0303 - val_loss: 0.0620 - val_mae: 0.1070 - val_mse: 0.0620\n",
      "Epoch 52/100\n",
      "443/443 [==============================] - 119s 268ms/step - loss: 0.0256 - mae: 0.0934 - mse: 0.0256 - val_loss: 0.0584 - val_mae: 0.1094 - val_mse: 0.0584\n",
      "Epoch 53/100\n",
      "443/443 [==============================] - 119s 268ms/step - loss: 0.0359 - mae: 0.0955 - mse: 0.0359 - val_loss: 0.0516 - val_mae: 0.1084 - val_mse: 0.0516\n",
      "Epoch 54/100\n",
      "443/443 [==============================] - 123s 277ms/step - loss: 0.0383 - mae: 0.0991 - mse: 0.0383 - val_loss: 0.0686 - val_mae: 0.1109 - val_mse: 0.0686\n",
      "Epoch 55/100\n",
      "443/443 [==============================] - 119s 269ms/step - loss: 0.0288 - mae: 0.0943 - mse: 0.0288 - val_loss: 0.0648 - val_mae: 0.1093 - val_mse: 0.0648\n",
      "Epoch 56/100\n",
      "443/443 [==============================] - 119s 270ms/step - loss: 0.0280 - mae: 0.0922 - mse: 0.0280 - val_loss: 0.0642 - val_mae: 0.1125 - val_mse: 0.0642\n",
      "finished training\n",
      "*------------------------------*\n",
      "\n",
      "starting evaluate\n",
      "*------------------------------*\n",
      "RMSE Score: 0.21906406056132946\n",
      "MAE Score: 0.32778638315977765\n",
      "finished evaluate\n",
      "*------------------------------*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_list = asset_df_B['full_path'].values\n",
    "target = asset_df_B['target'].values\n",
    "\n",
    "model_B = train(path_list, target, losses.mean_squared_error)\n",
    "# save_model(model_B, \"../models/swintransfofmerB.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c4e02b-f661-43bc-9ec9-ef0f4d627d73",
   "metadata": {},
   "source": [
    "## Evaluate model  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2302c3d7-1051-4c23-8717-aba374fd0d01",
   "metadata": {},
   "source": [
    "### Task A"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d163442f-e3aa-41b7-a28f-3f6e1ca1bbe4",
   "metadata": {},
   "source": [
    "file_name = \"../models/baselineA.pkl\"\n",
    "model = load_model(file_name)\n",
    "\n",
    "meta_features =\\\n",
    "    asset_df_A['collection.name'].unique().tolist() + ['num_sales']\n",
    "\n",
    "path_list = np.vstack(\n",
    "    (asset_df_A['full_path'].values.reshape(-1, 1),\n",
    "     asset_df_B['full_path'].values.reshape(-1, 1))\n",
    ").reshape(-1)\n",
    "meta_data = np.vstack(\n",
    "    (asset_df_A[meta_features].values.reshape(-1, len(meta_features)),\n",
    "     asset_df_B[meta_features].values.reshape(-1, len(meta_features)))\n",
    ")\n",
    "target = np.vstack(\n",
    "    (asset_df_A['target'].values.reshape(-1, 1),\n",
    "     asset_df_B['target'].values.reshape(-1, 1))\n",
    ").reshape(-1)\n",
    "\n",
    "train_path, val_path, train_meta, val_meta, train_y, val_y =\\\n",
    "    train_test_split(path_list, meta_data, target, test_size=0.1, random_state=6174)\n",
    "\n",
    "val_gen = FullPathDataLoader(path_list=val_path,\n",
    "                             meta_data=val_meta, target=val_y,\n",
    "                             batch_size=1, shuffle=False, is_train=False)\n",
    "\n",
    "model.evaluate(val_gen, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c61d60-b0ad-4ffa-8568-debe83ba19eb",
   "metadata": {},
   "source": [
    "### Task B"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76536ea4-0ac3-4bf7-aeed-9b9c9b219bd9",
   "metadata": {},
   "source": [
    "file_name = \"../models/baselineB.pkl\"\n",
    "model = load_model(file_name)\n",
    "\n",
    "path_list = asset_df_B['full_path'].values\n",
    "meta_data = asset_df_B[meta_features].values\n",
    "target = asset_df_B['target'].values\n",
    "\n",
    "train_path, val_path, train_meta, val_meta, train_y, val_y =\\\n",
    "    train_test_split(path_list, meta_data, target, test_size=0.1, random_state=6174)\n",
    "\n",
    "val_gen = FullPathDataLoader(path_list=val_path,\n",
    "                             meta_data=val_meta, target=val_y,\n",
    "                             batch_size=1, shuffle=False, is_train=False)\n",
    "\n",
    "model.evaluate(val_gen, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9d8f1-6140-4fb9-9369-f1cd005f0281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
