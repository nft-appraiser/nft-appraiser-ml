{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b83eaf3b-a15b-42fd-99da-dc9e6950f7bd",
   "metadata": {},
   "source": [
    "# モデル改善の試行錯誤用ノートブック  \n",
    "\n",
    "# 価格予測モデルのBaseline  \n",
    "- CNNを用いたモデルを作成する．  \n",
    "- 価格予測とクラス分類でタスクが大きく異なるので，imagenetで学習したモデルを用いないものを最初に作成する．  \n",
    "- サイトに載せられる画像を教師データとしており，画像が大きく回転したりなどは不要と考えられるためそのような前処理は行わない．  \n",
    "- 損失関数にはmaeもしくはrmseを用いる．  \n",
    "\n",
    "## モデルの構築  \n",
    "- EfficientNetB0（未学習）を用いて特徴量を抽出．  \n",
    "- num_sales, コレクション名のone-hotベクトルを抽出した特徴量に結合．  \n",
    "- 全結合層を重ねて出力．  \n",
    "- ImageNetを用いて事前学習したものとしていないもので比較する．  \n",
    "- 目的変数をそのまま予測するとスケールが大きすぎるので，先に対数変換して評価関数にRMSE, MAEなどを用いるほうが良いかも．  \n",
    "- **このノートブックでやっているのは事前学習有り．**  \n",
    "\n",
    "## 評価関数  \n",
    "- RMSLEを用いる．  \n",
    "$$RMSLE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (\\log{(y_i+1)} - \\log{(\\hat{y_i} +1)})^2}$$\n",
    "\n",
    "- 追加でMAPEを用いてみる．  \n",
    "$$MAPE = \\frac{100}{n} \\sum_{i=1}^n |\\frac{\\hat{y}_i - y_i}{y_i}|$$\n",
    "\n",
    "タスクAに関してはデータ不足の可能性が考えられるため，特徴量抽出とともにデータを追加で収集する．  \n",
    "\n",
    "## 変数（タスクA）  \n",
    "- 目的変数: last_sale.total_price  \n",
    "- 説明変数: 画像データ，コレクション名（collection.name），num_sales，\n",
    "\n",
    "## 変数（タスクB）  \n",
    "- 目的変数: last_sale.total_price  \n",
    "- 説明変数: 画像データ  \n",
    "\n",
    "### 実験結果  \n",
    "- 2021/11/16: ベースモデルをSwinTransformerにして学習\n",
    "    - SwinTransformerのGit: https://github.com/rishigami/Swin-Transformer-TF\n",
    "    - Apach Lisence 2.0のため商用利用可能  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd6e453-d7a2-447a-94c2-e3e41e7c841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "import math\n",
    "import tempfile\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.losses as losses\n",
    "import tensorflow.keras.optimizers as optim\n",
    "import tensorflow.keras.activations as activations\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "import tensorflow.keras.callbacks as callbacks\n",
    "from tensorflow.keras.applications import EfficientNetB0 as efn\n",
    "import cloudpickle\n",
    "\n",
    "sys.path.append(\"../Swin-Transformer-TF\")\n",
    "from swintransformer import SwinTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa2369f-6c62-4ec4-a3cd-6790e8794ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (3,27,28,71,88,119) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (21747, 169)\n",
      "data shape: (7861, 169)\n"
     ]
    }
   ],
   "source": [
    "A_IMGPATH = \"../data/taskA/img\"\n",
    "A_DFPATH = \"../data/taskA/table\"\n",
    "B_IMGPATH = \"../data/taskB/img\"\n",
    "B_DFPATH = \"../data/taskB/table\"\n",
    "asset_df_A = pd.read_csv(os.path.join(A_DFPATH, \"asset_data.csv\"))\n",
    "asset_df_B = pd.read_csv(os.path.join(B_DFPATH, \"asset_data.csv\"))\n",
    "\n",
    "asset_df_A = asset_df_A.rename(columns={\"last_sale.total_price\": \"target\"})\n",
    "asset_df_B = asset_df_B.rename(columns={\"last_sale.total_price\": \"target\"})\n",
    "\n",
    "asset_df_A = pd.concat((asset_df_A, pd.get_dummies(asset_df_A[\"collection.name\"])), axis=1)\n",
    "asset_df_B[asset_df_A[\"collection.name\"].unique()] = 0\n",
    "\n",
    "asset_df_A[\"full_path\"] =\\\n",
    "    asset_df_A[\"image_id\"].apply(lambda x: A_IMGPATH + \"/\" + x)\n",
    "asset_df_B[\"full_path\"] =\\\n",
    "    asset_df_B[\"image_id\"].apply(lambda x: B_IMGPATH + \"/\" + x)\n",
    "\n",
    "asset_df_A['target'] = asset_df_A['target'].astype(float) * 1e-18\n",
    "asset_df_B['target'] = asset_df_B['target'].astype(float) * 1e-18\n",
    "asset_df_A = asset_df_A.query('target > 0')\n",
    "asset_df_B = asset_df_B.query('target > 0')\n",
    "asset_df_A['target'] = asset_df_A['target'].apply(lambda x: np.log1p(x))\n",
    "asset_df_B['target'] = asset_df_B['target'].apply(lambda x: np.log1p(x))\n",
    "\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "print(f\"data shape: {asset_df_A.shape}\")\n",
    "print(f\"data shape: {asset_df_B.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9128f-90a3-4a76-a850-c81273b9967d",
   "metadata": {},
   "source": [
    "## Helper Functions  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2126e4f-9af4-4b15-9bde-dc1a5f8a1124",
   "metadata": {},
   "source": [
    "### DataLoader  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb23666-1673-4db8-85da-9f8aa0fcbd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullPathDataLoader(Sequence):\n",
    "    \"\"\"\n",
    "    Data loader that load images, meta data and targets.\n",
    "    This class is inherited Sequence class of Keras.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path_list: np.ndarray, target: Optional[np.ndarray] = None,\n",
    "                 meta_data: Optional[np.ndarray] = None, batch_size: int = 16,\n",
    "                 task: str = \"B\", width: int = 256, height: int = 256,\n",
    "                 resize: bool = True, shuffle: bool = True, is_train: bool = True):\n",
    "        \"\"\"\n",
    "        Constructor. This method determines class variables.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path_list : np.ndarray[str]\n",
    "            The array of absolute paths of images.\n",
    "        meta_data : np.ndarray[int]\n",
    "            One-hot vector of collections.\n",
    "        target : np.ndarray\n",
    "            Array of target variavles.\n",
    "        batch_size : int\n",
    "            Batch size used when model training.\n",
    "        task : str\n",
    "            Please determine this data loader will be used for task A or B(default=A).\n",
    "        width : int\n",
    "            Width of resized image.\n",
    "        height : int\n",
    "            Height of resize image.\n",
    "        resize : bool\n",
    "            Flag determine whether to resize.\n",
    "        shuffle : bool\n",
    "            Flag determine whether to shuffle on epoch end.\n",
    "        is_train : bool\n",
    "            Determine whether this data loader will be used training model.\n",
    "            if you won't this data loader, you have set 'is_train'=False.\n",
    "        \"\"\"\n",
    "        self.path_list = path_list\n",
    "        self.batch_size = batch_size\n",
    "        self.task = task\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.resize = resize\n",
    "        self.shuffle = shuffle\n",
    "        self.is_train = is_train\n",
    "        self.length = math.ceil(len(self.path_list) / self.batch_size)\n",
    "\n",
    "        if self.is_train:\n",
    "            self.target = target\n",
    "        if self.task == \"A\":\n",
    "            self.meta_data = meta_data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        self.length : data length\n",
    "        \"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def get_img(self, path_list: np.ndarray):\n",
    "        \"\"\"\n",
    "        Load image data and resize image if 'resize'=True.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path_liist : np.ndarray\n",
    "            The array of relative image paths from directory 'dir_name'.\n",
    "            Size of this array is 'batch_size'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        img_list : np.ndarray\n",
    "            The array of image data.\n",
    "            Size of an image is (width, height, 3) if 'resize'=True.\n",
    "        '\"\"\"\n",
    "        img_list = []\n",
    "        for path in path_list:\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.resize(img, (self.width, self.height))\n",
    "            img = img / 255.\n",
    "            img_list.append(img)\n",
    "\n",
    "        img_list = np.array(img_list)\n",
    "        return img_list\n",
    "\n",
    "    def _shuffle(self):\n",
    "        \"\"\"\n",
    "        Shuffle path_list, meta model.\n",
    "        If 'is_train' is True, target is shuffled in association path_list.\n",
    "        \"\"\"\n",
    "        idx = np.random.permutation(len(self.path_list))\n",
    "        self.path_list = self.path_list[idx]\n",
    "        if self.task == \"A\":\n",
    "            self.meta_data = self.meta_data[idx]\n",
    "        if self.is_train:\n",
    "            self.target = self.target[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_list = self.path_list[self.batch_size*idx:self.batch_size*(idx+1)]\n",
    "        img_list = self.get_img(path_list)\n",
    "        if self.is_train:\n",
    "            target_list = self.target[self.batch_size*idx:self.batch_size*(idx+1)]\n",
    "            if self.task == \"A\":\n",
    "                meta = self.meta_data[self.batch_size*idx:self.batch_size*(idx+1)]\n",
    "                return (img_list, meta), target_list\n",
    "            else:\n",
    "                return img_list, target_list\n",
    "        else:\n",
    "            if self.task == \"A\":\n",
    "                meta = self.meta_data[self.batch_size*idx:self.batch_size*(idx+1)]\n",
    "                return ((img_list, meta),)\n",
    "            else:\n",
    "                return img_list\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.is_train:\n",
    "            self._shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccabc2f6-749f-404a-a617-9f0507113d78",
   "metadata": {},
   "source": [
    "### seed settings  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44fab9da-4ce3-4548-ba52-29baf9c5e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(random_state=6174):\n",
    "    tf.random.set_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "    random.seed(random_state)\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab18eb9-6e4f-40f8-9ffc-c2b394b47870",
   "metadata": {},
   "source": [
    "### Create model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae81ae9f-aead-43ec-9c7b-906a40574366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape: Tuple[int], output_shape: int,\n",
    "                 activation, loss, meta_shape: Optional[int] = None,\n",
    "                 task: str = \"B\", learning_rate: float = 0.001,\n",
    "                 pretrain: bool = False) -> models.Model:\n",
    "    \"\"\"\n",
    "    The function for creating model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : int\n",
    "        Shape of input image data.\n",
    "    output_shape : int\n",
    "        Shape of model output.\n",
    "    activation : function\n",
    "        The activation function used hidden layers.\n",
    "    loss : function\n",
    "        The loss function of model.\n",
    "    meta_shape : int\n",
    "        Shape of input meta data of image.\n",
    "    task : str\n",
    "        Please determine this model will be used for task A or B(default=A).\n",
    "    learning_rate : float\n",
    "        The learning rate of model.\n",
    "    pretrain : bool\n",
    "        Flag that deterimine whether use pretrain model(default=False).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : keras.models.Model\n",
    "        Model instance.\n",
    "    \"\"\"\n",
    "    if pretrain:\n",
    "        weights = 'imagenet'\n",
    "    else:\n",
    "        weights = None\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    base_model = SwinTransformer('swin_tiny_224', include_top=False, pretrained=True, use_tpu=False)(inputs)\n",
    "\n",
    "    if task == \"A\":\n",
    "        meta_inputs = layers.Input(shape=meta_shape)\n",
    "        concate = layers.Concatenate()([base_model, meta_inputs])\n",
    "        dense1 = layers.Dense(units=128)(concate)\n",
    "        av1 = layers.Activation(activation)(dense1)\n",
    "        dr1 = layers.Dropout(0.3)(av1)\n",
    "        dense2 = layers.Dense(units=64)(dr1)\n",
    "        av2 = layers.Activation(activation)(dense2)\n",
    "        dr2 = layers.Dropout(0.3)(av2)\n",
    "        outputs = layers.Dense(output_shape)(dr2)\n",
    "\n",
    "        model = models.Model(inputs=[inputs, meta_inputs], outputs=[outputs])\n",
    "\n",
    "    elif task == \"B\":\n",
    "        dense1 = layers.Dense(units=128)(base_model)\n",
    "        av1 = layers.Activation(activation)(dense1)\n",
    "        dr1 = layers.Dropout(0.3)(av1)\n",
    "        dense2 = layers.Dense(units=64)(dr1)\n",
    "        av2 = layers.Activation(activation)(dense2)\n",
    "        dr2 = layers.Dropout(0.3)(av2)\n",
    "        outputs = layers.Dense(output_shape)(dr2)\n",
    "\n",
    "        model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Please set task is A or B.\")\n",
    "\n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=optim.Adam(learning_rate=learning_rate),\n",
    "                  metrics=['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d8ef5c-83cf-4e06-a76a-ebac170eab62",
   "metadata": {},
   "source": [
    "### Training model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d69c795d-f36c-45e1-88b4-f3c591e4f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(path_list: np.ndarray, target: np.ndarray, loss,\n",
    "          meta_data: Optional[np.ndarray] = None, task: str = \"B\"):\n",
    "    \"\"\"\n",
    "    The function for training model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_list : np.ndarray\n",
    "        The path list of all image data.\n",
    "    target : np.ndarray\n",
    "        The array of targets data.\n",
    "    loss : function\n",
    "        The loss function of keras.\n",
    "    meta_data : np.ndarray\n",
    "        The array of meta data of image.\n",
    "    task : str\n",
    "        Please determine you train model for task A or B(default=A).\n",
    "    \"\"\"\n",
    "    if task == \"A\":\n",
    "        train_path, val_path, train_meta, val_meta, train_y, val_y =\\\n",
    "            train_test_split(path_list, meta_data, target, test_size=0.1, random_state=6174)\n",
    "        train_gen = FullPathDataLoader(path_list=train_path, target=train_y,\n",
    "                                       meta_data=train_meta, batch_size=16,\n",
    "                                       width=224, height=224, task=task)\n",
    "        val_gen = FullPathDataLoader(path_list=val_path, target=train_y,\n",
    "                                     meta_data=val_meta, batch_size=1,\n",
    "                                     width=224, height=224, task=task)\n",
    "    elif task == \"B\":\n",
    "        train_path, val_path, train_y, val_y =\\\n",
    "            train_test_split(path_list, target, test_size=0.1, random_state=6174)\n",
    "        train_gen = FullPathDataLoader(path_list=train_path, target=train_y,\n",
    "                                       width=224, height=224, batch_size=16, task=task)\n",
    "        val_gen = FullPathDataLoader(path_list=val_path, target=val_y,\n",
    "                                     width=224, height=224, batch_size=1, task=task)\n",
    "    else:\n",
    "        raise Exception(\"Please set task is A or B\")\n",
    "\n",
    "    set_seed()\n",
    "    model = NFTModel(\n",
    "        model_func=create_model, input_shape=(224, 224, 3),\n",
    "        output_shape=1,activation=activations.relu, loss=loss,\n",
    "        meta_shape=len(meta_features), task=task,\n",
    "        learning_rate=0.00001, pretrain=True\n",
    "    )\n",
    "\n",
    "    ES = callbacks.EarlyStopping(monitor='val_loss', patience=10,\n",
    "                                 restore_best_weights=True)\n",
    "\n",
    "    print(\"starting training\")\n",
    "    print('*' + '-' * 30 + '*')\n",
    "\n",
    "    model.fit(train_gen, val_gen, epochs=100, batch_size=16,\n",
    "              callbacks=[ES])\n",
    "\n",
    "    print(\"finished training\")\n",
    "    print('*' + '-' * 30 + '*' + '\\n')\n",
    "\n",
    "    if task == \"A\":\n",
    "        val_gen = FullPathDataLoader(path_list=val_path, target=train_y,\n",
    "                                     meta_data=val_meta, batch_size=1, task=task,\n",
    "                                     width=224, height=224, shuffle=False, is_train=False)\n",
    "    else:\n",
    "        val_gen = FullPathDataLoader(path_list=val_path, target=train_y,\n",
    "                                     batch_size=1, task=task,\n",
    "                                     width=224, height=224, shuffle=False, is_train=False)\n",
    "    print(\"starting evaluate\")\n",
    "    print('*' + '-' * 30 + '*')\n",
    "\n",
    "    model.evaluate(val_gen, val_y)\n",
    "\n",
    "    print(\"finished evaluate\")\n",
    "    print('*' + '-' * 30 + '*' + '\\n')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8611a18a-51e3-4841-ac3b-3b3400d8ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFTModel(KerasRegressor):\n",
    "    \"\"\"\n",
    "    Model class.\n",
    "    This class is inherited KerasRegressor class of keras.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_func, input_shape, output_shape,\n",
    "                 activation, loss, meta_shape, task, learning_rate, pretrain):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        Prameters\n",
    "        ---------\n",
    "        model_func : function\n",
    "            The function for creating model.\n",
    "        \"\"\"\n",
    "        self.model_func = model_func\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.meta_shape = meta_shape\n",
    "        self.task = task\n",
    "        self.learning_rate = learning_rate\n",
    "        self.pretrain = pretrain\n",
    "        super().__init__(\n",
    "            build_fn=model_func(input_shape, output_shape,\n",
    "                                activation=activation, loss=loss,\n",
    "                                meta_shape=meta_shape, task=task,\n",
    "                                learning_rate=learning_rate, pretrain=pretrain)\n",
    "        )\n",
    "        self.model = self.build_fn\n",
    "\n",
    "    def __getstate__(self):\n",
    "        result = {'sk_params': self.sk_params,\n",
    "                  'model_func': self.model_func,\n",
    "                  'input_shape': self.input_shape,\n",
    "                  'output_shape': self.output_shape,\n",
    "                  'activation': self.activation,\n",
    "                  'loss': self.loss,\n",
    "                  'meta_shape': self.meta_shape,\n",
    "                  'task': self.task,\n",
    "                  'learning_rate': self.learning_rate,\n",
    "                  'pretrain': self.pretrain}\n",
    "        with tempfile.TemporaryDirectory() as dir:\n",
    "            if hasattr(self, 'model'):\n",
    "                self.model.save_weights(dir + '/output.h5')\n",
    "                with open(dir + '/output.h5', 'rb') as f:\n",
    "                    result['weights'] = f.read()\n",
    "        return result\n",
    "\n",
    "    def __setstate__(self, serialized):\n",
    "        self.sk_params = serialized['sk_params']\n",
    "        self.model_func = serialized['model_func']\n",
    "        self.input_shape = serialized['input_shape']\n",
    "        self.output_shape = serialized['output_shape']\n",
    "        self.activation = serialized['activation']\n",
    "        self.loss = serialized['loss']\n",
    "        self.meta_shape = serialized['meta_shape']\n",
    "        self.task = serialized['task']\n",
    "        self.learning_rate = serialized['learning_rate']\n",
    "        self.pretrain = serialized['pretrain']\n",
    "        self.model = self.model_func(\n",
    "                    self.input_shape, self.output_shape,\n",
    "                    activation=self.activation, loss=self.loss,\n",
    "                    meta_shape=self.meta_shape, task=self.task,\n",
    "                    learning_rate=self.learning_rate, pretrain=self.pretrain\n",
    "                )\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as dir:\n",
    "            weight_data = serialized.get('weights')\n",
    "            if weight_data:\n",
    "                with open(dir + '/input.h5', 'wb') as f:\n",
    "                    f.write(weight_data)\n",
    "                self.model.load_weights(dir + '/input.h5')\n",
    "\n",
    "    def fit(self, train_gen, val_gen, epochs, batch_size, callbacks=None):\n",
    "        \"\"\"\n",
    "        Training model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_gen : iterator\n",
    "            The generator of train data.\n",
    "        val_gen : iterator\n",
    "            The generator of validation data.\n",
    "        epochs : int\n",
    "            Number of epochs for training model.\n",
    "        batch_size : int\n",
    "            Size of batch for training model.\n",
    "        callbacks : list\n",
    "            The list of callbacks.\n",
    "            For example [EarlyStopping instance, ModelCheckpoint instance]\n",
    "        \"\"\"\n",
    "        self.model.fit(train_gen, epochs=epochs, batch_size=batch_size,\n",
    "                       validation_data=val_gen, callbacks=callbacks)\n",
    "\n",
    "    def evaluate(self, test_X, test_y):\n",
    "        \"\"\"\n",
    "        Evaluate model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        test_X : iterator\n",
    "            The generator of test data.\n",
    "        test_y : np.ndarray\n",
    "            The array of targets of test data.\n",
    "        \"\"\"\n",
    "        pred = self.model.predict(test_X)\n",
    "        pred = np.where(pred < 0, 0, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(test_y, pred))\n",
    "        mae = np.sqrt(mean_absolute_error(test_y, pred))\n",
    "\n",
    "        print(f\"RMSE Score: {rmse}\")\n",
    "        print(f\"MAE Score: {mae}\")\n",
    "\n",
    "    def predict(self, img_path: str, collection_name: str, num_sales: int,\n",
    "                task: str = \"B\"):\n",
    "        \"\"\"\n",
    "        Predict data using trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img_path : str\n",
    "            The path of image data.\n",
    "        collection_name : str\n",
    "            Name of collection of the NFT.\n",
    "        num_sales : int\n",
    "            Number of times the NFT sold.\n",
    "        \"\"\"\n",
    "        if task == \"A\":\n",
    "            collections = ['CryptoPunks',\n",
    "                           'Bored Ape Yacht Club',\n",
    "                           'Edifice by Ben Kovach',\n",
    "                           'Mutant Ape Yacht Club',\n",
    "                           'The Sandbox',\n",
    "                           'Divine Anarchy',\n",
    "                           'Cosmic Labs',\n",
    "                           'Parallel Alpha',\n",
    "                           'Art Wars | AW',\n",
    "                           'Neo Tokyo Identities',\n",
    "                           'Neo Tokyo Part 2 Vault Cards',\n",
    "                           'Cool Cats NFT',\n",
    "                           'CrypToadz by GREMPLIN',\n",
    "                           'BearXLabs',\n",
    "                           'Desperate ApeWives',\n",
    "                           'Decentraland',\n",
    "                           'Neo Tokyo Part 3 Item Caches',\n",
    "                           'Doodles',\n",
    "                           'The Doge Pound',\n",
    "                           'Playboy Rabbitars Official',\n",
    "                           'THE SHIBOSHIS',\n",
    "                           'THE REAL GOAT SOCIETY',\n",
    "                           'Sipherian Flash',\n",
    "                           'Party Ape | Billionaire Club',\n",
    "                           'Treeverse',\n",
    "                           'Angry Apes United',\n",
    "                           'CyberKongz',\n",
    "                           'Emblem Vault [Ethereum]',\n",
    "                           'Fat Ape Club',\n",
    "                           'VeeFriends',\n",
    "                           'JUNGLE FREAKS BY TROSLEY',\n",
    "                           'Meebits',\n",
    "                           'Furballs.com Official',\n",
    "                           'Kaiju Kingz',\n",
    "                           'Bears Deluxe',\n",
    "                           'PUNKS Comic',\n",
    "                           'Hor1zon Troopers',\n",
    "                           'Lazy Lions',\n",
    "                           'LOSTPOETS',\n",
    "                           'Chain Runners',\n",
    "                           'Chromie Squiggle by Snowfro',\n",
    "                           'MekaVerse',\n",
    "                           'Vox Collectibles',\n",
    "                           'MutantCats',\n",
    "                           'World of Women',\n",
    "                           'SuperFarm Genesis Series',\n",
    "                           'Eponym by ART AI',]\n",
    "            collection_dict = {\n",
    "                 collections[i]: i for i in range(len(collections))\n",
    "            }\n",
    "            meta_data = np.zeros(shape=(len(collection_dict)+1))\n",
    "            if collection_name in collection_dict.keys():\n",
    "                meta_data[collection_dict[collection_name]] = 1\n",
    "            meta_data[-1] = num_sales\n",
    "            meta_data = meta_data.reshape(1, -1)\n",
    "\n",
    "            img = cv2.resize(cv2.imread(img_path)/255., (224, 224))\n",
    "            img = img.reshape(1, 224, 224, 3)\n",
    "\n",
    "            pred = self.model.predict([img, meta_data])\n",
    "        elif task == \"B\":\n",
    "            img = cv2.resize(cv2.imread(img_path)/255., (224, 224))\n",
    "            img = img.reshape(1, 224, 224, 3)\n",
    "\n",
    "            pred = self.model.predict(img)\n",
    "        else:\n",
    "            raise Exception(\"Please set task is A or B\")\n",
    "\n",
    "        return pred[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "184a4688-00bf-47b8-8e57-635cb9f0f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(instance, file_name: str):\n",
    "    \"\"\"\n",
    "    Save model as pickle file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    instance : Class instance\n",
    "        The class instance you want to save as pickle file.\n",
    "    file_name : str\n",
    "        The absolute path of file saved the instance.\n",
    "    \"\"\"\n",
    "    with open(file_name, mode='wb') as f:\n",
    "        cloudpickle.dump(instance, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9261d8b-47c3-4e1c-8bfa-f98763d74a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file_name: str):\n",
    "    \"\"\"\n",
    "    Load the model file of pickle.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        The absolute path of the model file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : tf.keras.models.Model\n",
    "        Trained model object.\n",
    "    \"\"\"\n",
    "    with open(file_name, mode='rb') as f:\n",
    "        model = cloudpickle.load(f)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b4cb72-e675-4a37-93d2-c76038d38b58",
   "metadata": {},
   "source": [
    "## Training models  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cd7cea-06f2-4aec-ab36-61c49e607e53",
   "metadata": {},
   "source": [
    "### TaskA  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b78b6b44-cd24-4e9f-a8d5-ba586d808654",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 08:55:06.711895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-19 08:55:06.716515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-19 08:55:06.716917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-19 08:55:06.717585: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-19 08:55:06.718109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-19 08:55:06.718590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-19 08:55:06.718986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-19 08:55:06.988558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-19 08:55:06.988983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-19 08:55:06.989369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-19 08:55:06.989732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9740 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "*------------------------------*\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 08:55:10.560778: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-11-19 08:55:19.483270: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8204\n",
      "2021-11-19 08:55:21.671193: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1224/1224 [==============================] - 341s 269ms/step - loss: 5.7204 - mae: 0.8646 - mse: 5.7204 - val_loss: 97.2260 - val_mae: 1.3864 - val_mse: 97.2260\n",
      "Epoch 2/100\n",
      "1224/1224 [==============================] - 327s 267ms/step - loss: 2.8578 - mae: 0.6891 - mse: 2.8578 - val_loss: 90.9288 - val_mae: 1.4694 - val_mse: 90.9288\n",
      "Epoch 3/100\n",
      "1224/1224 [==============================] - 332s 272ms/step - loss: 8.3696 - mae: 0.6471 - mse: 8.3696 - val_loss: 54.6650 - val_mae: 1.4089 - val_mse: 54.6650\n",
      "Epoch 4/100\n",
      "1224/1224 [==============================] - 328s 268ms/step - loss: 5.6578 - mae: 0.6362 - mse: 5.6578 - val_loss: 43.6177 - val_mae: 1.4477 - val_mse: 43.6177\n",
      "Epoch 5/100\n",
      "1224/1224 [==============================] - 331s 271ms/step - loss: 6.4166 - mae: 0.6300 - mse: 6.4166 - val_loss: 32.1879 - val_mae: 1.3919 - val_mse: 32.1879\n",
      "Epoch 6/100\n",
      "1224/1224 [==============================] - 335s 274ms/step - loss: 3.1106 - mae: 0.5585 - mse: 3.1106 - val_loss: 28.0191 - val_mae: 1.3755 - val_mse: 28.0191\n",
      "Epoch 7/100\n",
      "1224/1224 [==============================] - 335s 274ms/step - loss: 1.4553 - mae: 0.5065 - mse: 1.4553 - val_loss: 28.1411 - val_mae: 1.3904 - val_mse: 28.1411\n",
      "Epoch 8/100\n",
      "1224/1224 [==============================] - 336s 274ms/step - loss: 2.5948 - mae: 0.4945 - mse: 2.5948 - val_loss: 35.0460 - val_mae: 1.3450 - val_mse: 35.0460\n",
      "Epoch 9/100\n",
      "1224/1224 [==============================] - 335s 274ms/step - loss: 2.2325 - mae: 0.4833 - mse: 2.2325 - val_loss: 41.0247 - val_mae: 1.4569 - val_mse: 41.0247\n",
      "Epoch 10/100\n",
      "1224/1224 [==============================] - 336s 274ms/step - loss: 1.4043 - mae: 0.4581 - mse: 1.4043 - val_loss: 36.0243 - val_mae: 1.3882 - val_mse: 36.0243\n",
      "Epoch 11/100\n",
      "1224/1224 [==============================] - 336s 275ms/step - loss: 5.7596 - mae: 0.4692 - mse: 5.7596 - val_loss: 30.1966 - val_mae: 1.3884 - val_mse: 30.1966\n",
      "Epoch 12/100\n",
      "1224/1224 [==============================] - 336s 274ms/step - loss: 2.5355 - mae: 0.4482 - mse: 2.5355 - val_loss: 35.4179 - val_mae: 1.3907 - val_mse: 35.4179\n",
      "Epoch 13/100\n",
      "1224/1224 [==============================] - 336s 274ms/step - loss: 2.1988 - mae: 0.4335 - mse: 2.1988 - val_loss: 27.9591 - val_mae: 1.3864 - val_mse: 27.9591\n",
      "Epoch 14/100\n",
      "1224/1224 [==============================] - 336s 274ms/step - loss: 1.3720 - mae: 0.4130 - mse: 1.3720 - val_loss: 35.9290 - val_mae: 1.4268 - val_mse: 35.9290\n",
      "Epoch 15/100\n",
      "1224/1224 [==============================] - 335s 274ms/step - loss: 1.7690 - mae: 0.4059 - mse: 1.7690 - val_loss: 42.1735 - val_mae: 1.4200 - val_mse: 42.1735\n",
      "Epoch 16/100\n",
      "1224/1224 [==============================] - 336s 274ms/step - loss: 4.0088 - mae: 0.4005 - mse: 4.0088 - val_loss: 34.0291 - val_mae: 1.3226 - val_mse: 34.0291\n",
      "Epoch 17/100\n",
      "1224/1224 [==============================] - 336s 274ms/step - loss: 1.6943 - mae: 0.3942 - mse: 1.6943 - val_loss: 33.1744 - val_mae: 1.3972 - val_mse: 33.1744\n",
      "Epoch 18/100\n",
      "1224/1224 [==============================] - 336s 275ms/step - loss: 8.3995 - mae: 0.4103 - mse: 8.3995 - val_loss: 27.3058 - val_mae: 1.3550 - val_mse: 27.3058\n",
      "Epoch 19/100\n",
      "1224/1224 [==============================] - 336s 274ms/step - loss: 0.7328 - mae: 0.3671 - mse: 0.7328 - val_loss: 30.4722 - val_mae: 1.4403 - val_mse: 30.4722\n",
      "Epoch 20/100\n",
      "1224/1224 [==============================] - 336s 275ms/step - loss: 3.7172 - mae: 0.3712 - mse: 3.7172 - val_loss: 23.5231 - val_mae: 1.3670 - val_mse: 23.5231\n",
      "Epoch 21/100\n",
      "1224/1224 [==============================] - 336s 274ms/step - loss: 1.2053 - mae: 0.3635 - mse: 1.2053 - val_loss: 26.8012 - val_mae: 1.3855 - val_mse: 26.8012\n",
      "Epoch 22/100\n",
      "1224/1224 [==============================] - 337s 275ms/step - loss: 1.0813 - mae: 0.3574 - mse: 1.0813 - val_loss: 23.7947 - val_mae: 1.4242 - val_mse: 23.7947\n",
      "Epoch 23/100\n",
      "1224/1224 [==============================] - 336s 275ms/step - loss: 1.5420 - mae: 0.3461 - mse: 1.5420 - val_loss: 22.2078 - val_mae: 1.4251 - val_mse: 22.2078\n",
      "Epoch 24/100\n",
      "1224/1224 [==============================] - 336s 275ms/step - loss: 1.9982 - mae: 0.3512 - mse: 1.9982 - val_loss: 19.7285 - val_mae: 1.4079 - val_mse: 19.7285\n",
      "Epoch 25/100\n",
      "1224/1224 [==============================] - 336s 274ms/step - loss: 0.6968 - mae: 0.3396 - mse: 0.6968 - val_loss: 20.2387 - val_mae: 1.3853 - val_mse: 20.2387\n",
      "Epoch 26/100\n",
      "1224/1224 [==============================] - 336s 275ms/step - loss: 1.8854 - mae: 0.3463 - mse: 1.8854 - val_loss: 19.0981 - val_mae: 1.4473 - val_mse: 19.0981\n",
      "Epoch 27/100\n",
      "1224/1224 [==============================] - 336s 275ms/step - loss: 0.6891 - mae: 0.3378 - mse: 0.6891 - val_loss: 17.5340 - val_mae: 1.3919 - val_mse: 17.5340\n",
      "Epoch 28/100\n",
      "1224/1224 [==============================] - 336s 274ms/step - loss: 0.6185 - mae: 0.3318 - mse: 0.6185 - val_loss: 16.4325 - val_mae: 1.3721 - val_mse: 16.4325\n",
      "Epoch 29/100\n",
      "1224/1224 [==============================] - 336s 275ms/step - loss: 0.7522 - mae: 0.3182 - mse: 0.7522 - val_loss: 19.2943 - val_mae: 1.3778 - val_mse: 19.2943\n",
      "Epoch 30/100\n",
      "1224/1224 [==============================] - 336s 275ms/step - loss: 1.6080 - mae: 0.3241 - mse: 1.6080 - val_loss: 14.7630 - val_mae: 1.3684 - val_mse: 14.7630\n",
      "Epoch 31/100\n",
      "1224/1224 [==============================] - 337s 275ms/step - loss: 0.4822 - mae: 0.3153 - mse: 0.4822 - val_loss: 17.1433 - val_mae: 1.3799 - val_mse: 17.1433\n",
      "Epoch 32/100\n",
      "1224/1224 [==============================] - 336s 274ms/step - loss: 1.9030 - mae: 0.3184 - mse: 1.9030 - val_loss: 13.9306 - val_mae: 1.3671 - val_mse: 13.9306\n",
      "Epoch 33/100\n",
      "1224/1224 [==============================] - 337s 275ms/step - loss: 5.3221 - mae: 0.3263 - mse: 5.3221 - val_loss: 10.7447 - val_mae: 1.3565 - val_mse: 10.7447\n",
      "Epoch 34/100\n",
      "1224/1224 [==============================] - 336s 274ms/step - loss: 0.9505 - mae: 0.3100 - mse: 0.9505 - val_loss: 9.9534 - val_mae: 1.3786 - val_mse: 9.9534\n",
      "Epoch 35/100\n",
      "1224/1224 [==============================] - 336s 275ms/step - loss: 0.6159 - mae: 0.3113 - mse: 0.6159 - val_loss: 9.6343 - val_mae: 1.3380 - val_mse: 9.6343\n",
      "Epoch 36/100\n",
      "1224/1224 [==============================] - 336s 274ms/step - loss: 0.4569 - mae: 0.3032 - mse: 0.4569 - val_loss: 9.3005 - val_mae: 1.3401 - val_mse: 9.3005\n",
      "Epoch 37/100\n",
      "1224/1224 [==============================] - 336s 274ms/step - loss: 0.4815 - mae: 0.2976 - mse: 0.4815 - val_loss: 11.4251 - val_mae: 1.3838 - val_mse: 11.4251\n",
      "Epoch 38/100\n",
      "1224/1224 [==============================] - 337s 275ms/step - loss: 0.3902 - mae: 0.2948 - mse: 0.3902 - val_loss: 14.2712 - val_mae: 1.3616 - val_mse: 14.2712\n",
      "Epoch 39/100\n",
      "1224/1224 [==============================] - 336s 274ms/step - loss: 0.3645 - mae: 0.2889 - mse: 0.3645 - val_loss: 16.4404 - val_mae: 1.3766 - val_mse: 16.4404\n",
      "Epoch 40/100\n",
      "1224/1224 [==============================] - 335s 274ms/step - loss: 0.3921 - mae: 0.2889 - mse: 0.3921 - val_loss: 19.3058 - val_mae: 1.3637 - val_mse: 19.3058\n",
      "Epoch 41/100\n",
      "1224/1224 [==============================] - 340s 278ms/step - loss: 0.9738 - mae: 0.2947 - mse: 0.9738 - val_loss: 16.9189 - val_mae: 1.3652 - val_mse: 16.9189\n",
      "Epoch 42/100\n",
      "1224/1224 [==============================] - 340s 278ms/step - loss: 0.7627 - mae: 0.2927 - mse: 0.7627 - val_loss: 13.4212 - val_mae: 1.4035 - val_mse: 13.4212\n",
      "Epoch 43/100\n",
      "1224/1224 [==============================] - 341s 279ms/step - loss: 0.7368 - mae: 0.2862 - mse: 0.7368 - val_loss: 11.2616 - val_mae: 1.3538 - val_mse: 11.2616\n",
      "Epoch 44/100\n",
      "1224/1224 [==============================] - 340s 278ms/step - loss: 2.8121 - mae: 0.2950 - mse: 2.8121 - val_loss: 8.6191 - val_mae: 1.3539 - val_mse: 8.6191\n",
      "Epoch 45/100\n",
      "1224/1224 [==============================] - 341s 279ms/step - loss: 0.4927 - mae: 0.2847 - mse: 0.4927 - val_loss: 12.2869 - val_mae: 1.3893 - val_mse: 12.2869\n",
      "Epoch 46/100\n",
      "1224/1224 [==============================] - 341s 279ms/step - loss: 0.6396 - mae: 0.2847 - mse: 0.6396 - val_loss: 13.1952 - val_mae: 1.3607 - val_mse: 13.1952\n",
      "Epoch 47/100\n",
      "1224/1224 [==============================] - 342s 279ms/step - loss: 0.4741 - mae: 0.2825 - mse: 0.4741 - val_loss: 17.9875 - val_mae: 1.3847 - val_mse: 17.9875\n",
      "Epoch 48/100\n",
      "1224/1224 [==============================] - 340s 278ms/step - loss: 0.8860 - mae: 0.2758 - mse: 0.8860 - val_loss: 13.0356 - val_mae: 1.3427 - val_mse: 13.0356\n",
      "Epoch 49/100\n",
      "1224/1224 [==============================] - 341s 279ms/step - loss: 0.4574 - mae: 0.2789 - mse: 0.4574 - val_loss: 11.3244 - val_mae: 1.3068 - val_mse: 11.3244\n",
      "Epoch 50/100\n",
      "1224/1224 [==============================] - 343s 280ms/step - loss: 0.9250 - mae: 0.2817 - mse: 0.9250 - val_loss: 13.2019 - val_mae: 1.3421 - val_mse: 13.2019\n",
      "Epoch 51/100\n",
      "1224/1224 [==============================] - 343s 280ms/step - loss: 0.3592 - mae: 0.2707 - mse: 0.3592 - val_loss: 15.2747 - val_mae: 1.4097 - val_mse: 15.2747\n",
      "Epoch 52/100\n",
      "1224/1224 [==============================] - 342s 279ms/step - loss: 1.0762 - mae: 0.2806 - mse: 1.0762 - val_loss: 15.8908 - val_mae: 1.3639 - val_mse: 15.8908\n",
      "Epoch 53/100\n",
      "1224/1224 [==============================] - 343s 280ms/step - loss: 0.9235 - mae: 0.2786 - mse: 0.9235 - val_loss: 14.3289 - val_mae: 1.3300 - val_mse: 14.3289\n",
      "Epoch 54/100\n",
      "1224/1224 [==============================] - 342s 280ms/step - loss: 0.4947 - mae: 0.2780 - mse: 0.4947 - val_loss: 13.0893 - val_mae: 1.3373 - val_mse: 13.0893\n",
      "finished training\n",
      "*------------------------------*\n",
      "\n",
      "starting evaluate\n",
      "*------------------------------*\n",
      "RMSE Score: 0.9496190053465577\n",
      "MAE Score: 0.5479366753420206\n",
      "finished evaluate\n",
      "*------------------------------*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_features =\\\n",
    "    asset_df_A['collection.name'].unique().tolist() + ['num_sales']\n",
    "\n",
    "path_list = asset_df_A['full_path'].values\n",
    "meta_data = asset_df_A[meta_features].values\n",
    "target = asset_df_A['target'].values\n",
    "\n",
    "model_A = train(path_list, target, losses.mean_squared_error, meta_data,\n",
    "                task=\"A\")\n",
    "# save_model(model_A, \"../models/baselineA.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4e9dc9-b1eb-462a-bd8c-a8b559d24e0d",
   "metadata": {},
   "source": [
    "### TaskA（画像のみ）  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcb07da4-d972-487b-90c1-a7d45e057d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "*------------------------------*\n",
      "Epoch 1/100\n",
      "1224/1224 [==============================] - 348s 277ms/step - loss: 1.1644 - mae: 0.6134 - mse: 1.1644 - val_loss: 0.7221 - val_mae: 0.4215 - val_mse: 0.7221\n",
      "Epoch 2/100\n",
      "1224/1224 [==============================] - 339s 277ms/step - loss: 0.9161 - mae: 0.4922 - mse: 0.9161 - val_loss: 0.6481 - val_mae: 0.3312 - val_mse: 0.6481\n",
      "Epoch 3/100\n",
      "1224/1224 [==============================] - 339s 277ms/step - loss: 0.7910 - mae: 0.4399 - mse: 0.7910 - val_loss: 0.6369 - val_mae: 0.3201 - val_mse: 0.6369\n",
      "Epoch 4/100\n",
      "1224/1224 [==============================] - 341s 279ms/step - loss: 0.7418 - mae: 0.4188 - mse: 0.7418 - val_loss: 0.6122 - val_mae: 0.3017 - val_mse: 0.6122\n",
      "Epoch 5/100\n",
      "1224/1224 [==============================] - 339s 277ms/step - loss: 0.7152 - mae: 0.4010 - mse: 0.7152 - val_loss: 0.6544 - val_mae: 0.3208 - val_mse: 0.6544\n",
      "Epoch 6/100\n",
      "1224/1224 [==============================] - 340s 277ms/step - loss: 0.6530 - mae: 0.3871 - mse: 0.6530 - val_loss: 0.5678 - val_mae: 0.2877 - val_mse: 0.5678\n",
      "Epoch 7/100\n",
      "1224/1224 [==============================] - 339s 277ms/step - loss: 0.6123 - mae: 0.3724 - mse: 0.6123 - val_loss: 0.5709 - val_mae: 0.2912 - val_mse: 0.5709\n",
      "Epoch 8/100\n",
      "1224/1224 [==============================] - 339s 277ms/step - loss: 0.5411 - mae: 0.3584 - mse: 0.5411 - val_loss: 0.6778 - val_mae: 0.3349 - val_mse: 0.6778\n",
      "Epoch 9/100\n",
      "1224/1224 [==============================] - 340s 278ms/step - loss: 0.5178 - mae: 0.3477 - mse: 0.5178 - val_loss: 0.6808 - val_mae: 0.2971 - val_mse: 0.6808\n",
      "Epoch 10/100\n",
      "1224/1224 [==============================] - 340s 278ms/step - loss: 0.4698 - mae: 0.3369 - mse: 0.4698 - val_loss: 0.6862 - val_mae: 0.3008 - val_mse: 0.6862\n",
      "Epoch 11/100\n",
      "1224/1224 [==============================] - 340s 278ms/step - loss: 0.4200 - mae: 0.3299 - mse: 0.4200 - val_loss: 0.7579 - val_mae: 0.2862 - val_mse: 0.7579\n",
      "Epoch 12/100\n",
      "1224/1224 [==============================] - 337s 275ms/step - loss: 0.3720 - mae: 0.3201 - mse: 0.3720 - val_loss: 0.7413 - val_mae: 0.2914 - val_mse: 0.7413\n",
      "Epoch 13/100\n",
      "1224/1224 [==============================] - 336s 275ms/step - loss: 0.3773 - mae: 0.3133 - mse: 0.3773 - val_loss: 0.7583 - val_mae: 0.2972 - val_mse: 0.7583\n",
      "Epoch 14/100\n",
      "1224/1224 [==============================] - 335s 274ms/step - loss: 0.3320 - mae: 0.3034 - mse: 0.3320 - val_loss: 0.9593 - val_mae: 0.3202 - val_mse: 0.9593\n",
      "Epoch 15/100\n",
      "1224/1224 [==============================] - 336s 274ms/step - loss: 0.2993 - mae: 0.2974 - mse: 0.2993 - val_loss: 0.9130 - val_mae: 0.2998 - val_mse: 0.9130\n",
      "Epoch 16/100\n",
      "1224/1224 [==============================] - 325s 265ms/step - loss: 0.2985 - mae: 0.2966 - mse: 0.2985 - val_loss: 0.8619 - val_mae: 0.3450 - val_mse: 0.8619\n",
      "finished training\n",
      "*------------------------------*\n",
      "\n",
      "starting evaluate\n",
      "*------------------------------*\n",
      "RMSE Score: 0.7535116876963924\n",
      "MAE Score: 0.5363799891870046\n",
      "finished evaluate\n",
      "*------------------------------*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_list = asset_df_A['full_path'].values\n",
    "target = asset_df_A['target'].values\n",
    "\n",
    "model_A = train(path_list, target, losses.mean_squared_error,\n",
    "                task=\"B\")\n",
    "save_model(model_A, \"../models/swintransformerA.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87444d2c-ae2a-4cc6-9776-6ab420b16ed4",
   "metadata": {},
   "source": [
    "### TaskB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abcc6969-e9b1-4aed-bde1-22975d18fb10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "*------------------------------*\n",
      "Epoch 1/100\n",
      "443/443 [==============================] - 122s 258ms/step - loss: 0.2787 - mae: 0.2877 - mse: 0.2787 - val_loss: 0.1290 - val_mae: 0.2057 - val_mse: 0.1290\n",
      "Epoch 2/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.2164 - mae: 0.2425 - mse: 0.2164 - val_loss: 0.1144 - val_mae: 0.1936 - val_mse: 0.1144\n",
      "Epoch 3/100\n",
      "443/443 [==============================] - 112s 254ms/step - loss: 0.1755 - mae: 0.2271 - mse: 0.1755 - val_loss: 0.1472 - val_mae: 0.2026 - val_mse: 0.1472\n",
      "Epoch 4/100\n",
      "443/443 [==============================] - 112s 254ms/step - loss: 0.1737 - mae: 0.2212 - mse: 0.1737 - val_loss: 0.1077 - val_mae: 0.1917 - val_mse: 0.1077\n",
      "Epoch 5/100\n",
      "443/443 [==============================] - 112s 254ms/step - loss: 0.1627 - mae: 0.2110 - mse: 0.1627 - val_loss: 0.1098 - val_mae: 0.1744 - val_mse: 0.1098\n",
      "Epoch 6/100\n",
      "443/443 [==============================] - 112s 254ms/step - loss: 0.1300 - mae: 0.2026 - mse: 0.1300 - val_loss: 0.1351 - val_mae: 0.1764 - val_mse: 0.1351\n",
      "Epoch 7/100\n",
      "443/443 [==============================] - 112s 254ms/step - loss: 0.1442 - mae: 0.1980 - mse: 0.1442 - val_loss: 0.0924 - val_mae: 0.1726 - val_mse: 0.0924\n",
      "Epoch 8/100\n",
      "443/443 [==============================] - 115s 260ms/step - loss: 0.1093 - mae: 0.1872 - mse: 0.1093 - val_loss: 0.0893 - val_mae: 0.1619 - val_mse: 0.0893\n",
      "Epoch 9/100\n",
      "443/443 [==============================] - 119s 268ms/step - loss: 0.1041 - mae: 0.1807 - mse: 0.1041 - val_loss: 0.0768 - val_mae: 0.1561 - val_mse: 0.0768\n",
      "Epoch 10/100\n",
      "443/443 [==============================] - 119s 267ms/step - loss: 0.0871 - mae: 0.1718 - mse: 0.0871 - val_loss: 0.0724 - val_mae: 0.1510 - val_mse: 0.0724\n",
      "Epoch 11/100\n",
      "443/443 [==============================] - 113s 256ms/step - loss: 0.0834 - mae: 0.1671 - mse: 0.0834 - val_loss: 0.0850 - val_mae: 0.1551 - val_mse: 0.0850\n",
      "Epoch 12/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0716 - mae: 0.1606 - mse: 0.0716 - val_loss: 0.0742 - val_mae: 0.1493 - val_mse: 0.0742\n",
      "Epoch 13/100\n",
      "443/443 [==============================] - 113s 256ms/step - loss: 0.0741 - mae: 0.1605 - mse: 0.0741 - val_loss: 0.0658 - val_mae: 0.1397 - val_mse: 0.0658\n",
      "Epoch 14/100\n",
      "443/443 [==============================] - 113s 256ms/step - loss: 0.0782 - mae: 0.1608 - mse: 0.0782 - val_loss: 0.0609 - val_mae: 0.1412 - val_mse: 0.0609\n",
      "Epoch 15/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0889 - mae: 0.1594 - mse: 0.0889 - val_loss: 0.0670 - val_mae: 0.1450 - val_mse: 0.0670\n",
      "Epoch 16/100\n",
      "443/443 [==============================] - 113s 256ms/step - loss: 0.0631 - mae: 0.1492 - mse: 0.0631 - val_loss: 0.0621 - val_mae: 0.1331 - val_mse: 0.0621\n",
      "Epoch 17/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0573 - mae: 0.1423 - mse: 0.0573 - val_loss: 0.0605 - val_mae: 0.1318 - val_mse: 0.0605\n",
      "Epoch 18/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0544 - mae: 0.1407 - mse: 0.0544 - val_loss: 0.0671 - val_mae: 0.1307 - val_mse: 0.0671\n",
      "Epoch 19/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0590 - mae: 0.1405 - mse: 0.0590 - val_loss: 0.0620 - val_mae: 0.1276 - val_mse: 0.0620\n",
      "Epoch 20/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0549 - mae: 0.1376 - mse: 0.0549 - val_loss: 0.0671 - val_mae: 0.1391 - val_mse: 0.0671\n",
      "Epoch 21/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0529 - mae: 0.1358 - mse: 0.0529 - val_loss: 0.0613 - val_mae: 0.1308 - val_mse: 0.0613\n",
      "Epoch 22/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0509 - mae: 0.1326 - mse: 0.0509 - val_loss: 0.0782 - val_mae: 0.1324 - val_mse: 0.0782\n",
      "Epoch 23/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0559 - mae: 0.1328 - mse: 0.0559 - val_loss: 0.0589 - val_mae: 0.1243 - val_mse: 0.0589\n",
      "Epoch 24/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0548 - mae: 0.1336 - mse: 0.0548 - val_loss: 0.0588 - val_mae: 0.1250 - val_mse: 0.0588\n",
      "Epoch 25/100\n",
      "443/443 [==============================] - 113s 256ms/step - loss: 0.0492 - mae: 0.1277 - mse: 0.0492 - val_loss: 0.0602 - val_mae: 0.1220 - val_mse: 0.0602\n",
      "Epoch 26/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0499 - mae: 0.1257 - mse: 0.0499 - val_loss: 0.0947 - val_mae: 0.1275 - val_mse: 0.0947\n",
      "Epoch 27/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0530 - mae: 0.1276 - mse: 0.0530 - val_loss: 0.0521 - val_mae: 0.1181 - val_mse: 0.0521\n",
      "Epoch 28/100\n",
      "443/443 [==============================] - 113s 256ms/step - loss: 0.0425 - mae: 0.1208 - mse: 0.0425 - val_loss: 0.0606 - val_mae: 0.1219 - val_mse: 0.0606\n",
      "Epoch 29/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0454 - mae: 0.1202 - mse: 0.0454 - val_loss: 0.0704 - val_mae: 0.1229 - val_mse: 0.0704\n",
      "Epoch 30/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0436 - mae: 0.1215 - mse: 0.0436 - val_loss: 0.0518 - val_mae: 0.1199 - val_mse: 0.0518\n",
      "Epoch 31/100\n",
      "443/443 [==============================] - 113s 256ms/step - loss: 0.0408 - mae: 0.1168 - mse: 0.0408 - val_loss: 0.0554 - val_mae: 0.1195 - val_mse: 0.0554\n",
      "Epoch 32/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0422 - mae: 0.1162 - mse: 0.0422 - val_loss: 0.0503 - val_mae: 0.1152 - val_mse: 0.0503\n",
      "Epoch 33/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0325 - mae: 0.1112 - mse: 0.0325 - val_loss: 0.0600 - val_mae: 0.1189 - val_mse: 0.0600\n",
      "Epoch 34/100\n",
      "443/443 [==============================] - 113s 256ms/step - loss: 0.0431 - mae: 0.1127 - mse: 0.0431 - val_loss: 0.0622 - val_mae: 0.1174 - val_mse: 0.0622\n",
      "Epoch 35/100\n",
      "443/443 [==============================] - 113s 256ms/step - loss: 0.0433 - mae: 0.1121 - mse: 0.0433 - val_loss: 0.0840 - val_mae: 0.1207 - val_mse: 0.0840\n",
      "Epoch 36/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0437 - mae: 0.1141 - mse: 0.0437 - val_loss: 0.0666 - val_mae: 0.1198 - val_mse: 0.0666\n",
      "Epoch 37/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0387 - mae: 0.1114 - mse: 0.0387 - val_loss: 0.0597 - val_mae: 0.1204 - val_mse: 0.0597\n",
      "Epoch 38/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0327 - mae: 0.1074 - mse: 0.0327 - val_loss: 0.0606 - val_mae: 0.1146 - val_mse: 0.0606\n",
      "Epoch 39/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0344 - mae: 0.1085 - mse: 0.0344 - val_loss: 0.0652 - val_mae: 0.1168 - val_mse: 0.0652\n",
      "Epoch 40/100\n",
      "443/443 [==============================] - 113s 255ms/step - loss: 0.0422 - mae: 0.1089 - mse: 0.0422 - val_loss: 0.0797 - val_mae: 0.1169 - val_mse: 0.0797\n",
      "Epoch 41/100\n",
      "443/443 [==============================] - 113s 256ms/step - loss: 0.0392 - mae: 0.1079 - mse: 0.0392 - val_loss: 0.0693 - val_mae: 0.1201 - val_mse: 0.0693\n",
      "Epoch 42/100\n",
      "443/443 [==============================] - 113s 256ms/step - loss: 0.0350 - mae: 0.1050 - mse: 0.0350 - val_loss: 0.0725 - val_mae: 0.1155 - val_mse: 0.0725\n",
      "finished training\n",
      "*------------------------------*\n",
      "\n",
      "starting evaluate\n",
      "*------------------------------*\n",
      "RMSE Score: 0.2243693004075551\n",
      "MAE Score: 0.3392654482178234\n",
      "finished evaluate\n",
      "*------------------------------*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_list = asset_df_B['full_path'].values\n",
    "target = asset_df_B['target'].values\n",
    "\n",
    "model_B = train(path_list, target, losses.mean_squared_error)\n",
    "save_model(model_B, \"../models/swintransforfmerB.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61090d70-395c-444e-9238-092c61703ddb",
   "metadata": {},
   "source": [
    "## Predict  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bbcf84-aa39-4afc-9b31-56866f6118d4",
   "metadata": {},
   "source": [
    "### TaskA  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "386abfcd-8d42-4a21-a5f3-078804a532f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = asset_df_A['full_path'].values\n",
    "target = asset_df_A['target'].values\n",
    "\n",
    "gen = FullPathDataLoader(path_list=path_list, target=target,\n",
    "                             batch_size=1, shuffle=False, is_train=False,\n",
    "                             task=\"B\", width=224, height=224)\n",
    "\n",
    "pred = model_A.model.predict(gen)\n",
    "asset_df_A['predict'] = pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f2eb80-9935-4789-9371-698dc552cbbc",
   "metadata": {},
   "source": [
    "### TaskB  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c87871a-098f-427e-9888-ea37df410c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = asset_df_B['full_path'].values\n",
    "target = asset_df_B['target'].values\n",
    "\n",
    "gen = FullPathDataLoader(path_list=path_list, target=target,\n",
    "                             batch_size=1, shuffle=False, is_train=False,\n",
    "                             task=\"B\", width=224, height=224)\n",
    "\n",
    "pred = model_B.model.predict(gen)\n",
    "asset_df_B['predict'] = pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c4e02b-f661-43bc-9ec9-ef0f4d627d73",
   "metadata": {},
   "source": [
    "## Evaluate model  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2302c3d7-1051-4c23-8717-aba374fd0d01",
   "metadata": {},
   "source": [
    "### Task A"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d163442f-e3aa-41b7-a28f-3f6e1ca1bbe4",
   "metadata": {},
   "source": [
    "file_name = \"../models/baselineA.pkl\"\n",
    "model = load_model(file_name)\n",
    "\n",
    "meta_features =\\\n",
    "    asset_df_A['collection.name'].unique().tolist() + ['num_sales']\n",
    "\n",
    "path_list = np.vstack(\n",
    "    (asset_df_A['full_path'].values.reshape(-1, 1),\n",
    "     asset_df_B['full_path'].values.reshape(-1, 1))\n",
    ").reshape(-1)\n",
    "meta_data = np.vstack(\n",
    "    (asset_df_A[meta_features].values.reshape(-1, len(meta_features)),\n",
    "     asset_df_B[meta_features].values.reshape(-1, len(meta_features)))\n",
    ")\n",
    "target = np.vstack(\n",
    "    (asset_df_A['target'].values.reshape(-1, 1),\n",
    "     asset_df_B['target'].values.reshape(-1, 1))\n",
    ").reshape(-1)\n",
    "\n",
    "train_path, val_path, train_meta, val_meta, train_y, val_y =\\\n",
    "    train_test_split(path_list, meta_data, target, test_size=0.1, random_state=6174)\n",
    "\n",
    "val_gen = FullPathDataLoader(path_list=val_path,\n",
    "                             meta_data=val_meta, target=val_y,\n",
    "                             batch_size=1, shuffle=False, is_train=False)\n",
    "\n",
    "model.evaluate(val_gen, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c61d60-b0ad-4ffa-8568-debe83ba19eb",
   "metadata": {},
   "source": [
    "### Task B"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76536ea4-0ac3-4bf7-aeed-9b9c9b219bd9",
   "metadata": {},
   "source": [
    "file_name = \"../models/baselineB.pkl\"\n",
    "model = load_model(file_name)\n",
    "\n",
    "path_list = asset_df_B['full_path'].values\n",
    "meta_data = asset_df_B[meta_features].values\n",
    "target = asset_df_B['target'].values\n",
    "\n",
    "train_path, val_path, train_meta, val_meta, train_y, val_y =\\\n",
    "    train_test_split(path_list, meta_data, target, test_size=0.1, random_state=6174)\n",
    "\n",
    "val_gen = FullPathDataLoader(path_list=val_path,\n",
    "                             meta_data=val_meta, target=val_y,\n",
    "                             batch_size=1, shuffle=False, is_train=False)\n",
    "\n",
    "model.evaluate(val_gen, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9d8f1-6140-4fb9-9369-f1cd005f0281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
